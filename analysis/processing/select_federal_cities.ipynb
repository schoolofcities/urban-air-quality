{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "from calendar import monthrange\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import FEDERAL_CITIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['NAPS_ID', 'Station_Name', 'Status', 'Location_Address', 'City', 'P/T',\n",
       "       'Postal_Code', 'Timezone', 'Latitude', 'Longitude', 'Elevation',\n",
       "       'Start_Year', 'End_Year', 'Combined_Stations', 'Inlet_Height',\n",
       "       'Network', 'SO2', 'CO', 'NO2', 'NO', 'NOX', 'O3', 'PM_25_Continuous',\n",
       "       'PM_10_Continuous', 'PM_2.5_RM', 'PM10-2.5', 'PM2.5_Speciation', 'VOC',\n",
       "       'Carbonyl', 'PAH', 'Site_Type', 'Urbanization', 'Neighbourhood',\n",
       "       'Land_Use', 'Scale', 'PC', 'CD', 'CSD', 'CMA/CA', 'AQMS_Airzone',\n",
       "       'Core_Site'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stations = pd.read_csv('../../data/raw/federal/metadata/StationsNAPS-StationsSNPA.csv')\n",
    "df_stations.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove stations which have old data only (before 2000) or aren't in our list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stations after federal cities filter: 320\n",
      "Stations after date filter: 174\n"
     ]
    }
   ],
   "source": [
    "# Filter 1: Keep only stations in federal cities\n",
    "# Create a set of (city, province) tuples for faster lookup\n",
    "federal_cities_set = set(FEDERAL_CITIES)\n",
    "\n",
    "# Filter stations to keep only those in federal cities\n",
    "df_filtered = df_stations[\n",
    "    df_stations.apply(lambda row: (row['City'], row['P/T']) in federal_cities_set, axis=1)\n",
    "].copy()  # Create a copy to avoid SettingWithCopyWarning\n",
    "\n",
    "print(f\"Stations after federal cities filter: {len(df_filtered)}\")\n",
    "\n",
    "# Filter 2: Remove stations with data exclusively before 2000\n",
    "# Convert date strings to datetime with explicit format to avoid parsing warnings\n",
    "df_filtered.loc[:, 'Start_Year_dt'] = pd.to_datetime(\n",
    "    df_filtered['Start_Year'], \n",
    "    format='%Y-%m-%d %I:%M:%S %p', \n",
    "    errors='coerce'\n",
    ")\n",
    "df_filtered.loc[:, 'End_Year_dt'] = pd.to_datetime(\n",
    "    df_filtered['End_Year'], \n",
    "    format='%Y-%m-%d %I:%M:%S %p', \n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "# Define cutoff date (January 1st, 2000)\n",
    "cutoff_date = pd.Timestamp('2000-01-01')\n",
    "\n",
    "# Keep stations if:\n",
    "# 1. End year is NaN (ongoing stations) AND start year is not NaN, OR\n",
    "# 2. End year exists and is >= 2000-01-01 (stations that operated after 2000)\n",
    "# This removes stations that have both start and end dates exclusively before 2000\n",
    "df_filtered = df_filtered[\n",
    "    ((df_filtered['End_Year_dt'].isna()) & (df_filtered['Start_Year_dt'].notna())) |  # Ongoing stations with valid start\n",
    "    (df_filtered['End_Year_dt'] >= cutoff_date)  # Stations ending on or after 2000\n",
    "]\n",
    "\n",
    "print(f\"Stations after date filter: {len(df_filtered)}\")\n",
    "\n",
    "# Clean up temporary columns\n",
    "df_filtered = df_filtered.drop(['Start_Year_dt', 'End_Year_dt'], axis=1)\n",
    "\n",
    "df_filtered.to_csv('../../data/raw/federal/metadata/stations_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load station metadata\n",
    "df_stations2 = pd.read_csv('../../data/raw/federal/metadata/stations_2.csv')\n",
    "valid_stations = set(df_stations2['NAPS_ID'].astype(str))  # fast lookup\n",
    "\n",
    "# Define pollutants and years\n",
    "pollutants = ['NO2', 'O3', 'PM25']\n",
    "years = range(2005, 2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing NO2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [04:20<00:00, 13.69s/it]\n",
      "100%|██████████| 228/228 [00:00<00:00, 279.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing O3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [05:22<00:00, 16.95s/it]\n",
      "100%|██████████| 228/228 [00:00<00:00, 246.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing PM25...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [06:12<00:00, 19.62s/it]\n",
      "100%|██████████| 228/228 [00:01<00:00, 220.36it/s]\n"
     ]
    }
   ],
   "source": [
    "coverage_data = {}\n",
    "\n",
    "for pollutant in pollutants:\n",
    "    print(f'Processing {pollutant}...')\n",
    "    \n",
    "    # Dictionary to hold monthly coverage\n",
    "    monthly_coverage = {} # keys: month string YYYY-MM, values: dict of station-city coverage\n",
    "\n",
    "    for year in tqdm(years):\n",
    "        file_path = f'../../data/raw/federal/{pollutant}/{pollutant}_{year}.csv'\n",
    "        if not os.path.exists(file_path):\n",
    "            continue\n",
    "        \n",
    "        # Read CSV skipping first 7 rows, headers are on 8th line\n",
    "        df = pd.read_csv(file_path, skiprows=7)\n",
    "        \n",
    "        # Filter rows to only valid stations\n",
    "        df = df[df['NAPS ID//Identifiant SNPA'].astype(str).isin(valid_stations)]\n",
    "        \n",
    "        # Identify hour columns\n",
    "        hour_cols = [col for col in df.columns if col.startswith('H')]\n",
    "        \n",
    "        # Create station-city pair column (station ID first)\n",
    "        df['station_city'] = df['NAPS ID//Identifiant SNPA'].astype(str) + '-' + df['City//Ville']\n",
    "        \n",
    "        # Iterate over each row (day)\n",
    "        for idx, row in df.iterrows(): # tqdm(df.iterrows(), total=len(df), desc=f'{pollutant} {year} Rows'):\n",
    "            date = pd.to_datetime(row['Date//Date'])\n",
    "            month_str = date.strftime('%Y-%m')\n",
    "            \n",
    "            if month_str not in monthly_coverage:\n",
    "                monthly_coverage[month_str] = {}\n",
    "            \n",
    "            station_city = row['station_city']\n",
    "            if station_city not in monthly_coverage[month_str]:\n",
    "                monthly_coverage[month_str][station_city] = {'count': 0, 'total': 0}\n",
    "            \n",
    "            # Count valid hours\n",
    "            valid_hours = row[hour_cols].apply(lambda x: 1 if pd.notna(x) and x not in [-999, 9999] else 0).sum()\n",
    "            monthly_coverage[month_str][station_city]['count'] += valid_hours\n",
    "            monthly_coverage[month_str][station_city]['total'] += len(hour_cols)\n",
    "    \n",
    "    # Build final DataFrame\n",
    "    all_station_cities = sorted({sc for month in monthly_coverage.values() for sc in month.keys()},\n",
    "                                key=lambda x: int(x.split('-')[0]))  # sort by station ID numerically\n",
    "    all_months = pd.date_range(start='2005-01-01', end='2023-12-31', freq='MS').strftime('%Y-%m')\n",
    "    \n",
    "    coverage_df = pd.DataFrame(index=all_months, columns=all_station_cities)\n",
    "    \n",
    "    for month in tqdm(all_months):\n",
    "        for sc in all_station_cities:\n",
    "            if month in monthly_coverage and sc in monthly_coverage[month]:\n",
    "                counts = monthly_coverage[month][sc]\n",
    "                coverage_pct = round(100 * counts['count'] / counts['total'], 2)\n",
    "                coverage_df.loc[month, sc] = coverage_pct\n",
    "            else:\n",
    "                coverage_df.loc[month, sc] = np.nan # No data\n",
    "    \n",
    "    # Save CSV\n",
    "    coverage_df.to_csv(f'../../data/raw/federal/metadata/coverage_{pollutant}.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "City-only CSV saved to ../../data/raw/federal/metadata/station_cities.csv\n"
     ]
    }
   ],
   "source": [
    "# Choose which pollutant CSV to open\n",
    "pollutant = 'PM25'  # change to O3 or PM25 if needed\n",
    "input_path = f'../../data/raw/federal/metadata/coverage_{pollutant}.csv'\n",
    "output_path = f'../../data/raw/federal/metadata/station_cities.csv'\n",
    "\n",
    "# Load the coverage CSV\n",
    "df = pd.read_csv(input_path, index_col=0)\n",
    "\n",
    "# Extract cities from columns (remove station IDs)\n",
    "cities = [col.split('-', 1)[1] for col in df.columns]\n",
    "\n",
    "# Remove duplicates while preserving order\n",
    "seen = set()\n",
    "unique_cities = []\n",
    "for city in cities:\n",
    "    if city not in seen:\n",
    "        seen.add(city)\n",
    "        unique_cities.append(city)\n",
    "\n",
    "# Build new DataFrame\n",
    "df_cities = pd.DataFrame(index=df.index)  # months as index\n",
    "for city in unique_cities:\n",
    "    df_cities[city] = np.nan  # initialize columns\n",
    "\n",
    "# Reset index so month becomes first column\n",
    "df_cities.reset_index(inplace=True)\n",
    "df_cities.rename(columns={'index': 'Month'}, inplace=True)\n",
    "\n",
    "# Save to CSV\n",
    "df_cities.to_csv(output_path, index=False)\n",
    "print(f'City-only CSV saved to {output_path}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

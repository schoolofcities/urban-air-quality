{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil import parser\n",
    "from calendar import monthrange\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from openaq import OpenAQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import FEDERAL_CITIES\n",
    "from constants import FEDERAL_LOCATION_IDS\n",
    "\n",
    "FEDERAL_CITIES = list(FEDERAL_LOCATION_IDS.keys())\n",
    "\n",
    "API_KEY = \"be9dde17e764a66a5352413ec62ab925e5b89a58d8fdc4711fc5ad460cdbd29c\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse pollutant data from 2005 to 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:30<00:00,  1.36it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load station-city mapping (preferred stations per city-month)\n",
    "df_station_map = pd.read_csv('../../data/raw/federal/metadata/station_cities.csv')\n",
    "df_station_map = df_station_map.rename(columns={'Unnamed: 0': 'month'})\n",
    "df_station_map['month'] = pd.to_datetime(df_station_map['month'])\n",
    "df_station_map = df_station_map.set_index('month')\n",
    "\n",
    "federal_cities = df_station_map.columns.tolist()\n",
    "\n",
    "# Create full hourly date range from 2005-01-01 to 2025-12-31 23:00\n",
    "date_range = pd.date_range(start=\"2005-01-01\", end=\"2025-12-31 23:00:00\", freq=\"h\")\n",
    "columns = ['pm25', 'no2', 'o3', 'pm25_3h_avg', 'no2_3h_avg', 'o3_3h_avg',\n",
    "           'aqhi_raw', 'aqhi_plus_raw', 'aqhi', 'aqhi_plus']\n",
    "\n",
    "df_empty = pd.DataFrame(index=date_range, columns=columns, dtype=float)\n",
    "\n",
    "output_dir = '../../data/processed/federal/hourly/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Initialize each city file\n",
    "for city in tqdm(federal_cities):\n",
    "    filename = f\"{output_dir}{city}.csv\"\n",
    "    df_empty.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing NO2 → no2 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [09:50<00:00, 31.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing O3 → o3 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [11:19<00:00, 35.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing PM25 → pm25 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [12:18<00:00, 38.87s/it]\n"
     ]
    }
   ],
   "source": [
    "pollutants = {'NO2': 'no2', 'O3': 'o3', 'PM25': 'pm25'}\n",
    "years = range(2005, 2024)  # 2005–2023\n",
    "\n",
    "for pollutant, colname in pollutants.items():\n",
    "    print(f\"\\n=== Processing {pollutant} → {colname} ===\")\n",
    "\n",
    "    for year in tqdm(years):\n",
    "        file_path = f'../../data/raw/federal/{pollutant}/{pollutant}_{year}.csv'\n",
    "        if not os.path.exists(file_path):\n",
    "            continue\n",
    "\n",
    "        # Read raw pollutant-year file\n",
    "        df = pd.read_csv(file_path, skiprows=7)\n",
    "\n",
    "        # Extract hourly columns\n",
    "        hour_cols = [c for c in df.columns if c.startswith('H')]\n",
    "\n",
    "        # Melt to long format\n",
    "        melted = df.melt(\n",
    "            id_vars=['Date//Date', 'NAPS ID//Identifiant SNPA'],\n",
    "            value_vars=hour_cols,\n",
    "            var_name='hour',\n",
    "            value_name=colname\n",
    "        )\n",
    "\n",
    "        # Parse datetime\n",
    "        melted['Date//Date'] = pd.to_datetime(melted['Date//Date'])\n",
    "        melted['hour'] = melted['hour'].str.extract(r'H(\\d+)').astype(int)\n",
    "        melted['datetime'] = (\n",
    "            melted['Date//Date'] +\n",
    "            pd.to_timedelta(melted['hour'], unit='h') +\n",
    "            pd.Timedelta(hours=1)\n",
    "        )\n",
    "        melted = melted.drop(columns=['hour', 'Date//Date'])\n",
    "\n",
    "        # Filter invalids\n",
    "        melted[colname] = pd.to_numeric(melted[colname], errors='coerce')\n",
    "        melted.loc[melted[colname].isin([-999, 9999]), colname] = np.nan\n",
    "\n",
    "        # Add month for join with station map\n",
    "        melted['month'] = melted['datetime'].dt.to_period(\"M\").dt.to_timestamp()\n",
    "        melted['station_id'] = melted['NAPS ID//Identifiant SNPA'].astype(int)\n",
    "\n",
    "        # Now we need: (station_id, month) → city\n",
    "        # Reshape df_station_map for a merge\n",
    "        df_long = df_station_map.stack().reset_index()\n",
    "        df_long.columns = ['month', 'city', 'station_id']\n",
    "        df_long['station_id'] = df_long['station_id'].dropna().astype(int)\n",
    "\n",
    "        # Merge pollutant values with city mapping\n",
    "        merged = melted.merge(df_long, on=['station_id', 'month'], how='inner')\n",
    "\n",
    "        # Now merged has datetime, pollutant col, city\n",
    "        # Group by city, then write once\n",
    "        for city, sub in merged.groupby('city'):\n",
    "            city_file = f\"{output_dir}{city}.csv\"\n",
    "            df_city = pd.read_csv(city_file, index_col=0, parse_dates=True)\n",
    "\n",
    "            df_city.loc[sub['datetime'], colname] = sub[colname].values\n",
    "\n",
    "            df_city.to_csv(city_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use OpenAQ to query for pollutant data over 2024 and 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/42 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:06<00:00,  6.80it/s]\n"
     ]
    }
   ],
   "source": [
    "client = OpenAQ(api_key=API_KEY)\n",
    "\n",
    "sensor_records = []\n",
    "\n",
    "for city, location_id in tqdm(FEDERAL_LOCATION_IDS.items()):\n",
    "    if location_id is None:\n",
    "        # placeholder for cities with no known location_id\n",
    "        sensor_records.append([city, '', '', '', '', ''])\n",
    "        continue\n",
    "\n",
    "    sensors = client.locations.sensors(location_id).results\n",
    "\n",
    "    for sensor in sensors:\n",
    "        param = sensor.parameter['name']\n",
    "        if param in ['pm25', 'o3', 'no2']:\n",
    "            sensor_records.append([\n",
    "                city,\n",
    "                location_id,\n",
    "                sensor.id,\n",
    "                param,\n",
    "                sensor.datetime_first['local'],\n",
    "                sensor.datetime_last['local'],\n",
    "            ])\n",
    "\n",
    "df_sensors = pd.DataFrame.from_records(\n",
    "    sensor_records,\n",
    "    columns=['city', 'location_id', 'sensor_id', 'pollutant', 'date_first', 'date_last']\n",
    ")\n",
    "df_sensors.to_csv('../../data/raw/federal/metadata/sensors.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [01:07<00:00, 16.90s/it]\n"
     ]
    }
   ],
   "source": [
    "def fetch_monthly_measurements(sensor_id, year, month):\n",
    "    \"\"\"Fetch all measurements for a given sensor, year, and month.\"\"\"\n",
    "    start_date = f\"{year}-{month:02d}-01\"\n",
    "    last_day = monthrange(year, month)[1]\n",
    "    end_date = f\"{year}-{month:02d}-{last_day}T23:59:59\"\n",
    "\n",
    "    response = client.measurements.list(\n",
    "        sensors_id=sensor_id,\n",
    "        limit=1000,\n",
    "        page=1,\n",
    "        datetime_from=start_date,\n",
    "        datetime_to=end_date,\n",
    "    )\n",
    "\n",
    "    results = response.results\n",
    "    if not results:\n",
    "        return []\n",
    "\n",
    "    return results\n",
    "\n",
    "# Process measurements and update CSVs\n",
    "for city, location_id, sensor_id, pollutant, date_start, date_end in tqdm(sensor_records[100:]):\n",
    "    if not sensor_id:\n",
    "        continue\n",
    "\n",
    "    measurements = []\n",
    "    for year in [2024, 2025]:\n",
    "        for month in range(1, 13):\n",
    "            if year == 2025 and month >= 11:\n",
    "                continue\n",
    "            monthly_results = fetch_monthly_measurements(sensor_id, year, month)\n",
    "            # print(f'Fetched {year}-{month} with {len(monthly_results)} entries')\n",
    "            measurements.extend(monthly_results)\n",
    "\n",
    "    # Reorganize into a DataFrame\n",
    "    records = []\n",
    "    for item in measurements:\n",
    "        dt = parser.isoparse(item.period.datetime_to.local).replace(tzinfo=None)\n",
    "\n",
    "        # Convert values (µg/m³ for PM2.5; ppm → ppb for O3 and NO2)\n",
    "        val = item.value if pollutant == 'pm25' else item.value * 1000\n",
    "\n",
    "        records.append((dt, val))\n",
    "\n",
    "    if not records:\n",
    "        continue\n",
    "\n",
    "    df_new = pd.DataFrame(records, columns=['datetime', pollutant])\n",
    "    df_new = df_new.set_index('datetime').sort_index()\n",
    "\n",
    "    # Deduplicate: keep the last value per timestamp (you can also use 'mean')\n",
    "    df_new = df_new.groupby(df_new.index).last()\n",
    "\n",
    "    # Load the existing city CSV\n",
    "    city_file = f\"../../data/processed/federal/hourly/{city}.csv\"\n",
    "    df_city = pd.read_csv(city_file, index_col=0, parse_dates=True)\n",
    "\n",
    "    # Clear old values for 2024–2025\n",
    "    mask = (df_city.index >= \"2024-01-01\") & (df_city.index < \"2026-01-01\")\n",
    "    df_city.loc[mask, pollutant] = np.nan\n",
    "\n",
    "    # Align and overwrite values explicitly instead of df.update and save\n",
    "    df_city.loc[df_new.index, pollutant] = df_new[pollutant]\n",
    "    df_city.to_csv(city_file)\n",
    "    # print(f\"Updated {city} for {pollutant} (2024–2025)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute rolling averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [01:01<00:00,  1.46s/it]\n"
     ]
    }
   ],
   "source": [
    "POLLUTANTS = ['pm25', 'no2', 'o3']\n",
    "\n",
    "for city in tqdm(FEDERAL_CITIES):\n",
    "    city_path = f'../../data/processed/federal/hourly/{city}.csv'\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(city_path, index_col=0, parse_dates=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {city_path}: {e}\")\n",
    "        continue\n",
    "\n",
    "    for pollutant in POLLUTANTS:\n",
    "        avg_col = f\"{pollutant}_3h_avg\"\n",
    "        df[avg_col] = (\n",
    "            df[pollutant]\n",
    "            .rolling(window=3, min_periods=2)\n",
    "            .mean()\n",
    "            .round(1)  \n",
    "        )\n",
    "\n",
    "    # Save updated DataFrame\n",
    "    df.to_csv(city_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute AQHI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [01:03<00:00,  1.51s/it]\n"
     ]
    }
   ],
   "source": [
    "def compute_mAQI(no2, o3):\n",
    "    \"\"\"Compute Ontario mAQI from 1-hour NO2 (ppb) and O3 (ppb).\"\"\"\n",
    "    \n",
    "    def sub_index_no2(val):\n",
    "        if pd.isna(val):\n",
    "            return np.nan\n",
    "        if 0 <= val <= 110:\n",
    "            return 0.02264 * val + 1.000\n",
    "        elif 111 <= val <= 200:\n",
    "            return 0.03360 * val - 0.2291\n",
    "        elif 201 <= val <= 524:\n",
    "            return 0.01235 * val + 4.017\n",
    "        else:  # > 524\n",
    "            return 0.01810 * val + 1.000\n",
    "\n",
    "    def sub_index_o3(val):\n",
    "        if pd.isna(val):\n",
    "            return np.nan\n",
    "        if 0 <= val <= 50:\n",
    "            return 0.04980 * val + 1.000\n",
    "        elif 51 <= val <= 80:\n",
    "            return 0.1031 * val - 1.758\n",
    "        elif 81 <= val <= 149:\n",
    "            return 0.05868 * val + 1.747\n",
    "        else:  # > 149\n",
    "            return 0.05868 * val + 1.747\n",
    "\n",
    "    no2_idx = no2.apply(sub_index_no2)\n",
    "    o3_idx = o3.apply(sub_index_o3)\n",
    "\n",
    "    return pd.concat([no2_idx, o3_idx], axis=1).max(axis=1)  # keep decimals for AQHI+ comparison\n",
    "\n",
    "def compute_aqhi(pm25_3h, no2_3h, o3_3h):\n",
    "    \"\"\"Compute federal AQHI from 3-hour averages.\"\"\"\n",
    "    aqhi = (\n",
    "        1000 * (\n",
    "            (np.exp(0.000871 * no2_3h) - 1) +\n",
    "            (np.exp(0.000537 * o3_3h) - 1) +\n",
    "            (np.exp(0.000487 * pm25_3h) - 1)\n",
    "        )\n",
    "    ) / 10.4\n",
    "    return aqhi\n",
    "\n",
    "for city in tqdm(FEDERAL_CITIES):\n",
    "    city_path = f'../../data/processed/federal/hourly/{city}.csv'\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(city_path, index_col=0, parse_dates=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {city_path}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # --- Compute mAQI from 1-hour O3 & NO2 ---\n",
    "    mAQI = compute_mAQI(df['no2'], df['o3'])\n",
    "\n",
    "    # --- Compute federal AQHI from 3-hour averages ---\n",
    "    aqhi_raw = compute_aqhi(df['pm25_3h_avg'], df['no2_3h_avg'], df['o3_3h_avg'])\n",
    "\n",
    "    # Round AQHI for reporting (per guidelines)\n",
    "    aqhi_report = aqhi_raw.round().astype('Int64')\n",
    "    \n",
    "    # Cap AQHI values for public display (11 == \"10+\")\n",
    "    aqhi_report_final = aqhi_report.clip(upper=11)\n",
    "\n",
    "    # --- Compute AQHI Plus ---\n",
    "    aqhi_plus = aqhi_report.copy()\n",
    "    \n",
    "    # Step 1: Apply mAQI substitution\n",
    "    mask_maqi = (mAQI > 6) & ((mAQI > aqhi_raw) | aqhi_raw.isna())\n",
    "    aqhi_plus[mask_maqi] = mAQI[mask_maqi].round().astype('Int64')\n",
    "    \n",
    "    # Step 2: Apply PM2.5 trigger (April 2024 change)\n",
    "    # pm25 here is the 1-hour value in µg/m³\n",
    "    sub_pm25 = np.ceil(df['pm25'] / 10)\n",
    "    \n",
    "    mask_pm25 = sub_pm25 > aqhi_plus\n",
    "    aqhi_plus[mask_pm25] = sub_pm25[mask_pm25].astype('Int64')\n",
    "\n",
    "    # Cap AQHI Plus values for public display (11 == \"10+\")\n",
    "    aqhi_plus_final = aqhi_plus.clip(upper=11)\n",
    "\n",
    "    # --- Store results ---\n",
    "    df['aqhi_raw'] = aqhi_report\n",
    "    df['aqhi'] = aqhi_report_final\n",
    "    df['aqhi_plus_raw'] = aqhi_plus\n",
    "    df['aqhi_plus'] = aqhi_plus_final\n",
    "\n",
    "    # Save updated file\n",
    "    df.to_csv(city_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute daily AQHI values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing aqhi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [03:59<00:00,  5.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing aqhi_plus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [04:00<00:00,  5.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing pm25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [04:12<00:00,  6.02s/it]\n"
     ]
    }
   ],
   "source": [
    "daily_dates = pd.date_range(start=\"2005-01-01\", end=\"2025-12-31\", freq=\"D\")\n",
    "\n",
    "# Metrics to compute for each case\n",
    "metrics = ['4pm', 'min', 'max', 'mean', 'med']\n",
    "\n",
    "# Define cases: (col, raw_col, output_dir)\n",
    "cases = {\n",
    "    'aqhi': ('aqhi', 'aqhi_raw', '../../data/processed/federal/daily-aqhi'),\n",
    "    'aqhi_plus': ('aqhi_plus', 'aqhi_plus_raw', '../../data/processed/federal/daily-aqhi-plus'),\n",
    "    'pm25': ('pm25', 'pm25', '../../data/processed/federal/daily-pm25'),\n",
    "}\n",
    "\n",
    "for case_name, (col, raw_col, out_dir) in cases.items():\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"Processing {case_name}\")\n",
    "    for city in tqdm(FEDERAL_CITIES):\n",
    "        hourly_path = f'../../data/processed/federal/hourly/{city}.csv'\n",
    "        daily_path = f'{out_dir}/{city}.csv'\n",
    "\n",
    "        if not os.path.exists(hourly_path):\n",
    "            print(f\"No hourly file for {city}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            df_hourly = pd.read_csv(hourly_path, index_col=0, parse_dates=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {hourly_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        if col not in df_hourly.columns:\n",
    "            print(f\"No {col} column for {city}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Empty daily DataFrame\n",
    "        df_daily = pd.DataFrame(\n",
    "            index=daily_dates,\n",
    "            columns=[f\"{m}_{col}\" for m in metrics],\n",
    "            dtype=float\n",
    "        )\n",
    "\n",
    "        # Group by day\n",
    "        grouped = df_hourly.groupby(df_hourly.index.date)\n",
    "\n",
    "        for date, group in grouped:\n",
    "            date = pd.Timestamp(date)\n",
    "\n",
    "            # 4pm value\n",
    "            four_pm_val = group.loc[group.index.hour == 16, col]\n",
    "            df_daily.at[date, f'4pm_{col}'] = four_pm_val.iloc[0] if not four_pm_val.empty else np.nan\n",
    "\n",
    "            # Other metrics\n",
    "            vals = group[col].dropna()\n",
    "            vals_raw = group[raw_col].dropna()\n",
    "            if not vals.empty:\n",
    "                df_daily.at[date, f'min_{col}'] = vals.min()\n",
    "                df_daily.at[date, f'max_{col}'] = vals.max()\n",
    "                df_daily.at[date, f'mean_{col}'] = np.clip(vals_raw.mean().round(1), None, 11) if col != 'pm25' else vals_raw.mean().round(1)\n",
    "                df_daily.at[date, f'med_{col}'] = vals.median()\n",
    "\n",
    "        df_daily.to_csv(daily_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the coverage of the data - percent of each pollutant in each month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [03:18<00:00,  4.73s/it]\n",
      "100%|██████████| 252/252 [00:00<00:00, 1474.25it/s]\n",
      "100%|██████████| 21/21 [00:00<00:00, 1530.10it/s]\n"
     ]
    }
   ],
   "source": [
    "# Required columns\n",
    "REQUIRED_COLS = [\n",
    "    'pm25', 'no2', 'o3',\n",
    "    'pm25_3h_avg', 'no2_3h_avg', 'o3_3h_avg',\n",
    "    'aqhi', 'aqhi_plus', 'aqhi_raw', 'aqhi_plus_raw'\n",
    "]\n",
    "\n",
    "def compute_coverage_for_city(city_name):\n",
    "    \"\"\"Compute monthly and yearly coverage for a single city\"\"\"\n",
    "    # Load processed city data\n",
    "    path = f'../../data/processed/federal/hourly/{city_name}.csv'\n",
    "    df = pd.read_csv(path, index_col=0, parse_dates=True)\n",
    "\n",
    "    # Ensure all required columns exist\n",
    "    for col in REQUIRED_COLS:\n",
    "        if col not in df.columns:\n",
    "            df[col] = np.nan\n",
    "\n",
    "    # Build full month and year index\n",
    "    month_index = pd.date_range(start='2005-01-01', end='2025-12-31', freq='MS')\n",
    "    coverage_monthly = pd.DataFrame(index=month_index, columns=REQUIRED_COLS)\n",
    "\n",
    "    # Compute monthly coverage\n",
    "    for month_start in month_index:\n",
    "        month_end = month_start + pd.offsets.MonthEnd(1)\n",
    "        month_hours = pd.date_range(start=month_start, end=month_end, freq='h')\n",
    "        expected_count = len(month_hours)\n",
    "\n",
    "        for col in REQUIRED_COLS:\n",
    "            if month_start < df.index.min() or month_start > df.index.max():\n",
    "                coverage_monthly.loc[month_start, col] = 0.00\n",
    "            else:\n",
    "                mask = (df.index >= month_start) & (df.index <= month_end)\n",
    "                observed_count = df.loc[mask, col].notna().sum()\n",
    "                coverage_monthly.loc[month_start, col] = (observed_count / expected_count) * 100\n",
    "\n",
    "    # Round monthly coverage\n",
    "    coverage_monthly = coverage_monthly.astype(float).round(2)\n",
    "\n",
    "    # Compute yearly coverage by resampling and round\n",
    "    coverage_yearly = coverage_monthly.resample(\"YS\").mean().round(2)\n",
    "    coverage_yearly.index = coverage_yearly.index.year\n",
    "\n",
    "    return coverage_monthly, coverage_yearly\n",
    "\n",
    "def process_all_cities():\n",
    "    \"\"\"Compute and save coverage for all cities, and aggregate into overall monthly/yearly coverage\"\"\"\n",
    "    monthly_all_observed = {}\n",
    "    monthly_all_expected = {}\n",
    "    yearly_all_observed = {}\n",
    "    yearly_all_expected = {}\n",
    "\n",
    "    # Ensure output directories exist\n",
    "    os.makedirs(\"../../data/processed/federal/metadata/coverage/monthly\", exist_ok=True)\n",
    "    os.makedirs(\"../../data/processed/federal/metadata/coverage/yearly\", exist_ok=True)\n",
    "\n",
    "    # Process each city\n",
    "    for city in tqdm(FEDERAL_CITIES):\n",
    "        monthly, yearly = compute_coverage_for_city(city)\n",
    "\n",
    "        # Save per-city coverage\n",
    "        monthly.to_csv(f'../../data/processed/federal/metadata/coverage/monthly/{city}.csv')\n",
    "        yearly.to_csv(f'../../data/processed/federal/metadata/coverage/yearly/{city}.csv')\n",
    "\n",
    "        # For overall stats, we need observed+expected counts, not percentages\n",
    "        for freq, cov_df, obs_dict, exp_dict in [\n",
    "            (\"monthly\", monthly, monthly_all_observed, monthly_all_expected),\n",
    "            (\"yearly\", yearly, yearly_all_observed, yearly_all_expected),\n",
    "        ]:\n",
    "            for idx, row in cov_df.iterrows():\n",
    "                if freq == \"monthly\":\n",
    "                    start = pd.Timestamp(idx)\n",
    "                    end = start + pd.offsets.MonthEnd(1)\n",
    "                    expected_count = len(pd.date_range(start, end, freq='h'))\n",
    "                else:  # yearly\n",
    "                    start = pd.Timestamp(str(idx))\n",
    "                    end = start + pd.offsets.YearEnd(0)\n",
    "                    expected_count = len(pd.date_range(start, end, freq='h'))\n",
    "\n",
    "                if idx not in obs_dict:\n",
    "                    obs_dict[idx] = {col: 0 for col in REQUIRED_COLS}\n",
    "                    exp_dict[idx] = {col: 0 for col in REQUIRED_COLS}\n",
    "\n",
    "                for col in REQUIRED_COLS:\n",
    "                    # observed counts (percentage * expected_count / 100)\n",
    "                    observed_count = (row[col] / 100.0) * expected_count\n",
    "                    obs_dict[idx][col] += observed_count\n",
    "                    exp_dict[idx][col] += expected_count\n",
    "\n",
    "    # Build overall monthly coverage\n",
    "    monthly_all = pd.DataFrame(index=sorted(monthly_all_observed.keys()), columns=REQUIRED_COLS)\n",
    "    for idx in tqdm(monthly_all.index, total=len(monthly_all)):\n",
    "        for col in REQUIRED_COLS:\n",
    "            monthly_all.loc[idx, col] = round(\n",
    "                (monthly_all_observed[idx][col] / monthly_all_expected[idx][col]) * 100, 2\n",
    "            )\n",
    "    monthly_all.to_csv(\"../../data/processed/federal/metadata/coverage/monthly/all.csv\")\n",
    "\n",
    "    # Build overall yearly coverage\n",
    "    yearly_all = pd.DataFrame(index=sorted(yearly_all_observed.keys()), columns=REQUIRED_COLS)\n",
    "    for idx in tqdm(yearly_all.index, total=len(yearly_all)):\n",
    "        for col in REQUIRED_COLS:\n",
    "            yearly_all.loc[idx, col] = round(\n",
    "                (yearly_all_observed[idx][col] / yearly_all_expected[idx][col]) * 100, 2\n",
    "            )\n",
    "    yearly_all.to_csv(\"../../data/processed/federal/metadata/coverage/yearly/all.csv\")\n",
    "\n",
    "    return monthly_all, yearly_all\n",
    "\n",
    "monthly_all, yearly_all = process_all_cities()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

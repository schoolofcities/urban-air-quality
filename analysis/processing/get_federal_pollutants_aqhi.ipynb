{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "from calendar import monthrange\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from openaq import OpenAQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import FEDERAL_CITIES\n",
    "from constants import FEDERAL_LOCATION_IDS\n",
    "\n",
    "API_KEY = \"be9dde17e764a66a5352413ec62ab925e5b89a58d8fdc4711fc5ad460cdbd29c\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse pollutant data from 2005 to 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:30<00:00,  1.36it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load station-city mapping (preferred stations per city-month)\n",
    "df_station_map = pd.read_csv('../../data/raw/federal/metadata/station_cities.csv')\n",
    "df_station_map = df_station_map.rename(columns={'Unnamed: 0': 'month'})\n",
    "df_station_map['month'] = pd.to_datetime(df_station_map['month'])\n",
    "df_station_map = df_station_map.set_index('month')\n",
    "\n",
    "federal_cities = df_station_map.columns.tolist()\n",
    "\n",
    "# Create full hourly date range from 2005-01-01 to 2025-12-31 23:00\n",
    "date_range = pd.date_range(start=\"2005-01-01\", end=\"2025-12-31 23:00:00\", freq=\"h\")\n",
    "columns = ['pm25', 'no2', 'o3', 'pm25_3h_avg', 'no2_3h_avg', 'o3_3h_avg',\n",
    "           'aqhi_raw', 'aqhi_plus_raw', 'aqhi', 'aqhi_plus']\n",
    "\n",
    "df_empty = pd.DataFrame(index=date_range, columns=columns, dtype=float)\n",
    "\n",
    "output_dir = '../../data/processed/federal/hourly/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Initialize each city file\n",
    "for city in tqdm(federal_cities):\n",
    "    filename = f\"{output_dir}{city}.csv\"\n",
    "    df_empty.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing NO2 → no2 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [09:50<00:00, 31.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing O3 → o3 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [11:19<00:00, 35.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing PM25 → pm25 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [12:18<00:00, 38.87s/it]\n"
     ]
    }
   ],
   "source": [
    "pollutants = {'NO2': 'no2', 'O3': 'o3', 'PM25': 'pm25'}\n",
    "years = range(2005, 2024)  # 2005–2023\n",
    "\n",
    "for pollutant, colname in pollutants.items():\n",
    "    print(f\"\\n=== Processing {pollutant} → {colname} ===\")\n",
    "\n",
    "    for year in tqdm(years):\n",
    "        file_path = f'../../data/raw/federal/{pollutant}/{pollutant}_{year}.csv'\n",
    "        if not os.path.exists(file_path):\n",
    "            continue\n",
    "\n",
    "        # Read raw pollutant-year file\n",
    "        df = pd.read_csv(file_path, skiprows=7)\n",
    "\n",
    "        # Extract hourly columns\n",
    "        hour_cols = [c for c in df.columns if c.startswith('H')]\n",
    "\n",
    "        # Melt to long format\n",
    "        melted = df.melt(\n",
    "            id_vars=['Date//Date', 'NAPS ID//Identifiant SNPA'],\n",
    "            value_vars=hour_cols,\n",
    "            var_name='hour',\n",
    "            value_name=colname\n",
    "        )\n",
    "\n",
    "        # Parse datetime\n",
    "        melted['Date//Date'] = pd.to_datetime(melted['Date//Date'])\n",
    "        melted['hour'] = melted['hour'].str.extract(r'H(\\d+)').astype(int)\n",
    "        melted['datetime'] = (\n",
    "            melted['Date//Date'] +\n",
    "            pd.to_timedelta(melted['hour'], unit='h') +\n",
    "            pd.Timedelta(hours=1)\n",
    "        )\n",
    "        melted = melted.drop(columns=['hour', 'Date//Date'])\n",
    "\n",
    "        # Filter invalids\n",
    "        melted[colname] = pd.to_numeric(melted[colname], errors='coerce')\n",
    "        melted.loc[melted[colname].isin([-999, 9999]), colname] = np.nan\n",
    "\n",
    "        # Add month for join with station map\n",
    "        melted['month'] = melted['datetime'].dt.to_period(\"M\").dt.to_timestamp()\n",
    "        melted['station_id'] = melted['NAPS ID//Identifiant SNPA'].astype(int)\n",
    "\n",
    "        # Now we need: (station_id, month) → city\n",
    "        # Reshape df_station_map for a merge\n",
    "        df_long = df_station_map.stack().reset_index()\n",
    "        df_long.columns = ['month', 'city', 'station_id']\n",
    "        df_long['station_id'] = df_long['station_id'].dropna().astype(int)\n",
    "\n",
    "        # Merge pollutant values with city mapping\n",
    "        merged = melted.merge(df_long, on=['station_id', 'month'], how='inner')\n",
    "\n",
    "        # Now merged has datetime, pollutant col, city\n",
    "        # Group by city, then write once\n",
    "        for city, sub in merged.groupby('city'):\n",
    "            city_file = f\"{output_dir}{city}.csv\"\n",
    "            df_city = pd.read_csv(city_file, index_col=0, parse_dates=True)\n",
    "\n",
    "            df_city.loc[sub['datetime'], colname] = sub[colname].values\n",
    "\n",
    "            df_city.to_csv(city_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use OpenAQ to query for pollutant data over 2024 and 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:06<00:00,  6.24it/s]\n"
     ]
    }
   ],
   "source": [
    "client = OpenAQ(api_key=API_KEY)\n",
    "\n",
    "sensor_records = []\n",
    "\n",
    "for city, location_id in tqdm(FEDERAL_LOCATION_IDS.items()):\n",
    "    if location_id is None:\n",
    "        sensor_records.append([city, '', '', '', '', ''])\n",
    "        continue\n",
    "\n",
    "    sensors = client.locations.sensors(location_id).results\n",
    "\n",
    "    for sensor in sensors:\n",
    "        if sensor.parameter['name'] in ['pm25', 'o3', 'no2']:\n",
    "            sensor_records.append([\n",
    "                city, \n",
    "                location_id, \n",
    "                sensor.id,\n",
    "                sensor.parameter['name'],\n",
    "                sensor.datetime_first['local'],\n",
    "                sensor.datetime_last['local'],\n",
    "            ])\n",
    "\n",
    "df = pd.DataFrame.from_records(sensor_records, columns=['city', 'location_id', 'sensor_id', 'pollutant', 'date_first', 'date_last'])\n",
    "df.to_csv('../../data/raw/federal/metadata/sensors.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "St. John's\n"
     ]
    }
   ],
   "source": [
    "for city, location_id, sensor_id, pollutant, date_start, date_end in sensor_records:\n",
    "    if not sensor_id:\n",
    "        continue\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAQ(api_key=API_KEY)\n",
    "response = client.measurements.list(\n",
    "    sensors_id=25148, \n",
    "    limit=1000,\n",
    "    page=13,\n",
    "    datetime_from='2024-01-01',  # need to do 2024-2025, then 2025-2026 \n",
    "    datetime_to='2026-01-01',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1346572/3699858157.py:14: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  end_date = datetime.utcnow().isoformat() + \"Z\"\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Measurements.list() got an unexpected keyword argument 'location_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m all_results\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Fetch data\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_all_measurements\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_date\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Convert to DataFrame\u001b[39;00m\n\u001b[1;32m     43\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data)\n",
      "Cell \u001b[0;32mIn[12], line 22\u001b[0m, in \u001b[0;36mfetch_all_measurements\u001b[0;34m(location_id, start_date, end_date, limit)\u001b[0m\n\u001b[1;32m     19\u001b[0m all_results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 22\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeasurements\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocation_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocation_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Corrected parameter name\u001b[39;49;00m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdate_from\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_date\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdate_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend_date\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpage\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     results \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m results:\n",
      "\u001b[0;31mTypeError\u001b[0m: Measurements.list() got an unexpected keyword argument 'location_id'"
     ]
    }
   ],
   "source": [
    "from openaq import OpenAQ\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize client with API key\n",
    "API_KEY = \"0d33157cc065493f21b56c03c868fead2ba85358f4c6d1cffa7e1e91dd560f50\"\n",
    "client = OpenAQ(api_key=API_KEY)\n",
    "\n",
    "# Station ID\n",
    "location_id = 774\n",
    "\n",
    "# Date range\n",
    "start_date = '2024-01-01T00:00:00Z'\n",
    "end_date = datetime.utcnow().isoformat() + \"Z\"\n",
    "\n",
    "# Function to fetch all measurements with pagination\n",
    "def fetch_all_measurements(location_id, start_date, end_date, limit=1000):\n",
    "    page = 1\n",
    "    all_results = []\n",
    "\n",
    "    while True:\n",
    "        response = client.measurements.list(\n",
    "            location_id=location_id,  # Corrected parameter name\n",
    "            date_from=start_date,\n",
    "            date_to=end_date,\n",
    "            limit=limit,\n",
    "            page=page\n",
    "        )\n",
    "        results = response['results']\n",
    "        if not results:\n",
    "            break\n",
    "        \n",
    "        all_results.extend(results)\n",
    "        print(f\"Fetched page {page} with {len(results)} records\")\n",
    "        page += 1\n",
    "\n",
    "    return all_results\n",
    "\n",
    "# Fetch data\n",
    "data = fetch_all_measurements(location_id, start_date, end_date)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Optional: show first few rows\n",
    "print(df.head())\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"station_774_measurements.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

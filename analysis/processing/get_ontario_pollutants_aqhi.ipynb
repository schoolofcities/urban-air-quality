{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cdef680-d4ed-4b04-9d06-d78f57c9cfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e0411c1-76e6-435c-a98f-d4dfe7b46530",
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import ONTARIO_CITIES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9952f05b-52c4-40c8-981a-2e59c33a9009",
   "metadata": {},
   "source": [
    "## Parse and organize the pollutant data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4558a04e-37b8-49d7-affb-43be50fd974d",
   "metadata": {},
   "source": [
    "Create each of the output files with empty values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "894d66ea-4fd4-403b-adb0-0f3a7b85125c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 43/43 [00:14<00:00,  2.97it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create date range for every hour from 2005 to 2024, and empty df\n",
    "date_range = pd.date_range(start='2005-01-01', end='2024-12-31 23:00:00', freq='h')\n",
    "\n",
    "columns = ['pm25', 'no2', 'o3', 'pm25_3h_avg', 'no2_3h_avg', 'o3_3h_avg', 'aqhi']\n",
    "df_empty = pd.DataFrame(index=date_range, columns=columns)\n",
    "\n",
    "# Save a copy for each city\n",
    "output_dir = '../../data/processed/ontario/hourly/'\n",
    "for city in tqdm(ONTARIO_CITIES):\n",
    "    filename = f\"{output_dir}{city}.csv\"\n",
    "    df_empty.to_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8787e707-fcae-42de-ae62-e182fc2a1e0f",
   "metadata": {},
   "source": [
    "Parse data for each year and each pollutant from 2005-2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "15acda9d-099a-4bb8-b039-5c13b3506a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_city_df(city_name):\n",
    "    path = f'../../data/processed/ontario/hourly/{city_name}.csv'\n",
    "    return pd.read_csv(path, index_col=0, parse_dates=True)\n",
    "\n",
    "\n",
    "def parse_city_block(city_data_rows, header_row):\n",
    "    try:\n",
    "        header_trimmed = header_row[:27]\n",
    "        rows_trimmed = [row[:27] for row in city_data_rows]\n",
    "\n",
    "        df = pd.DataFrame(rows_trimmed)\n",
    "        df.columns = header_trimmed\n",
    "        df = df[['Date'] + [f'H{h:02d}' for h in range(1, 25)]]\n",
    "        df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "        df = df.dropna(subset=['Date'])\n",
    "\n",
    "        melted = df.melt(id_vars='Date', var_name='hour', value_name='value')\n",
    "        melted['hour'] = melted['hour'].str.extract(r'H(\\d{2})').astype(int) - 1\n",
    "        melted['datetime'] = melted['Date'] + pd.to_timedelta(melted['hour'], unit='h')\n",
    "        melted.set_index('datetime', inplace=True)\n",
    "\n",
    "        melted['value'] = pd.to_numeric(melted['value'], errors='coerce')\n",
    "        melted.loc[melted['value'].isin([-999, 9999]), 'value'] = np.nan\n",
    "\n",
    "        return melted\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing city block: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def update_city_df(city_df, melted, pollutant):\n",
    "    try:\n",
    "        city_df.loc[melted.index, pollutant] = melted['value']\n",
    "        return city_df\n",
    "    except Exception as e:\n",
    "        print(f\"Error updating city DataFrame: {e}\")\n",
    "        return city_df\n",
    "\n",
    "\n",
    "def process_city_block(city_name, header_row, city_data_rows, pollutant):\n",
    "    try:\n",
    "        city_df = load_city_df(city_name)\n",
    "        melted = parse_city_block(city_data_rows, header_row)\n",
    "        if melted is not None:\n",
    "            updated_df = update_city_df(city_df, melted, pollutant)\n",
    "            updated_df.to_csv(f'../../data/processed/ontario/hourly/{city_name}.csv')\n",
    "            # print(f\"✅ Updated {city_name}.csv with {pollutant} data.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to process {city_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "00038570-ea3e-4e90-88cb-aa2c6c6fcddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\n",
      "  0%|                                                    | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|██▏                                         | 1/20 [00:27<08:38, 27.28s/it]\u001b[A\n",
      " 10%|████▍                                       | 2/20 [00:54<08:14, 27.46s/it]\u001b[A\n",
      " 15%|██████▌                                     | 3/20 [01:21<07:41, 27.17s/it]\u001b[A\n",
      " 20%|████████▊                                   | 4/20 [01:48<07:13, 27.10s/it]\u001b[A\n",
      " 25%|███████████                                 | 5/20 [02:17<06:55, 27.67s/it]\u001b[A\n",
      " 30%|█████████████▏                              | 6/20 [02:46<06:34, 28.20s/it]\u001b[A\n",
      " 35%|███████████████▍                            | 7/20 [03:19<06:27, 29.82s/it]\u001b[A\n",
      " 40%|█████████████████▌                          | 8/20 [03:50<06:01, 30.12s/it]\u001b[A\n",
      " 45%|███████████████████▊                        | 9/20 [04:21<05:34, 30.40s/it]\u001b[A\n",
      " 50%|█████████████████████▌                     | 10/20 [04:58<05:22, 32.29s/it]\u001b[A\n",
      " 55%|███████████████████████▋                   | 11/20 [05:29<04:48, 32.08s/it]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 12/20 [05:58<04:08, 31.06s/it]\u001b[A\n",
      " 65%|███████████████████████████▉               | 13/20 [06:25<03:30, 30.01s/it]\u001b[A\n",
      " 70%|██████████████████████████████             | 14/20 [06:54<02:58, 29.70s/it]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 15/20 [07:24<02:28, 29.77s/it]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 16/20 [07:56<02:00, 30.21s/it]\u001b[A\n",
      " 85%|████████████████████████████████████▌      | 17/20 [08:25<01:29, 29.95s/it]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 18/20 [08:57<01:01, 30.63s/it]\u001b[A\n",
      " 95%|████████████████████████████████████████▊  | 19/20 [09:28<00:30, 30.60s/it]\u001b[A\n",
      "100%|███████████████████████████████████████████| 20/20 [09:56<00:00, 29.81s/it]\u001b[A\n",
      " 33%|██████████████▋                             | 1/3 [09:56<19:52, 596.27s/it]\n",
      "  0%|                                                    | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|██▏                                         | 1/20 [00:26<08:32, 26.97s/it]\u001b[A\n",
      " 10%|████▍                                       | 2/20 [00:55<08:18, 27.72s/it]\u001b[A\n",
      " 15%|██████▌                                     | 3/20 [01:28<08:33, 30.22s/it]\u001b[A\n",
      " 20%|████████▊                                   | 4/20 [01:57<07:57, 29.82s/it]\u001b[A\n",
      " 25%|███████████                                 | 5/20 [02:26<07:23, 29.59s/it]\u001b[A\n",
      " 30%|█████████████▏                              | 6/20 [02:57<06:57, 29.82s/it]\u001b[A\n",
      " 35%|███████████████▍                            | 7/20 [03:36<07:08, 32.96s/it]\u001b[A\n",
      " 40%|█████████████████▌                          | 8/20 [04:09<06:36, 33.05s/it]\u001b[A\n",
      " 45%|███████████████████▊                        | 9/20 [04:42<06:01, 32.87s/it]\u001b[A\n",
      " 50%|█████████████████████▌                     | 10/20 [05:30<06:17, 37.77s/it]\u001b[A\n",
      " 55%|███████████████████████▋                   | 11/20 [06:17<06:04, 40.51s/it]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 12/20 [06:50<05:05, 38.19s/it]\u001b[A\n",
      " 65%|███████████████████████████▉               | 13/20 [07:19<04:08, 35.44s/it]\u001b[A\n",
      " 70%|██████████████████████████████             | 14/20 [07:50<03:24, 34.13s/it]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 15/20 [08:22<02:46, 33.30s/it]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 16/20 [08:51<02:08, 32.10s/it]\u001b[A\n",
      " 85%|████████████████████████████████████▌      | 17/20 [09:28<01:40, 33.55s/it]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 18/20 [10:02<01:07, 33.71s/it]\u001b[A\n",
      " 95%|████████████████████████████████████████▊  | 19/20 [10:34<00:33, 33.25s/it]\u001b[A\n",
      "100%|███████████████████████████████████████████| 20/20 [11:07<00:00, 33.36s/it]\u001b[A\n",
      " 67%|█████████████████████████████▎              | 2/3 [21:03<10:37, 637.95s/it]\n",
      "  0%|                                                    | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|██▏                                         | 1/20 [00:22<07:13, 22.80s/it]\u001b[A\n",
      " 10%|████▍                                       | 2/20 [00:47<07:09, 23.86s/it]\u001b[A\n",
      " 15%|██████▌                                     | 3/20 [01:13<07:03, 24.88s/it]\u001b[A\n",
      " 20%|████████▊                                   | 4/20 [01:38<06:40, 25.00s/it]\u001b[A\n",
      " 25%|███████████                                 | 5/20 [02:05<06:22, 25.50s/it]\u001b[A\n",
      " 30%|█████████████▏                              | 6/20 [02:33<06:12, 26.62s/it]\u001b[A\n",
      " 35%|███████████████▍                            | 7/20 [03:01<05:52, 27.08s/it]\u001b[A\n",
      " 40%|█████████████████▌                          | 8/20 [03:29<05:28, 27.38s/it]\u001b[A\n",
      " 45%|███████████████████▊                        | 9/20 [03:57<05:02, 27.49s/it]\u001b[A\n",
      " 50%|█████████████████████▌                     | 10/20 [04:26<04:40, 28.03s/it]\u001b[A\n",
      " 55%|███████████████████████▋                   | 11/20 [04:57<04:20, 28.97s/it]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 12/20 [05:29<03:58, 29.79s/it]\u001b[A\n",
      " 65%|███████████████████████████▉               | 13/20 [05:59<03:29, 29.87s/it]\u001b[A\n",
      " 70%|██████████████████████████████             | 14/20 [06:32<03:03, 30.62s/it]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 15/20 [07:12<02:47, 33.53s/it]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 16/20 [07:45<02:13, 33.40s/it]\u001b[A\n",
      " 85%|████████████████████████████████████▌      | 17/20 [08:15<01:37, 32.49s/it]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 18/20 [08:49<01:05, 32.85s/it]\u001b[A\n",
      " 95%|████████████████████████████████████████▊  | 19/20 [09:24<00:33, 33.60s/it]\u001b[A\n",
      "100%|███████████████████████████████████████████| 20/20 [09:54<00:00, 29.73s/it]\u001b[A\n",
      "100%|████████████████████████████████████████████| 3/3 [30:58<00:00, 619.34s/it]\n"
     ]
    }
   ],
   "source": [
    "pollutants = ['pm25', 'o3', 'no2']\n",
    "years = list(range(2005, 2025))\n",
    "\n",
    "for pollutant in pollutants:\n",
    "    for year in tqdm(years):\n",
    "        file_path = f'../../data/raw/ontario/{pollutant}/{year}_{pollutant}.csv'\n",
    "\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"⚠️ File not found: {file_path}\")\n",
    "            continue\n",
    "\n",
    "        with open(file_path, newline='', encoding='utf-8') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            current_city = None\n",
    "            header_row = []\n",
    "            city_data_rows = []\n",
    "\n",
    "            for row in reader:\n",
    "                if not row:\n",
    "                    continue\n",
    "\n",
    "                first_cell = row[0].strip().lower()\n",
    "\n",
    "                if first_cell in ONTARIO_CITIES:\n",
    "                    # Process the previous block if exists\n",
    "                    if current_city and city_data_rows and header_row:\n",
    "                        try:\n",
    "                            process_city_block(current_city, header_row, city_data_rows, pollutant)\n",
    "                        except Exception as e:\n",
    "                            print(f\"❌ Failed to process {current_city} in {file_path}: {e}\")\n",
    "\n",
    "                    # Start a new block\n",
    "                    current_city = first_cell\n",
    "                    header_row = []\n",
    "                    city_data_rows = []\n",
    "                    continue\n",
    "\n",
    "                if current_city:\n",
    "                    if len(row) > 2 and row[2].strip().lower() == 'date':\n",
    "                        header_row = row\n",
    "                    elif header_row and len(row) >= 27:\n",
    "                        city_data_rows.append(row)\n",
    "\n",
    "            # Process last city block after file ends\n",
    "            if current_city and city_data_rows and header_row:\n",
    "                try:\n",
    "                    process_city_block(current_city, header_row, city_data_rows, pollutant)\n",
    "                except Exception as e:\n",
    "                    print(f\"❌ Failed to process final block for {current_city} in {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed247e66-ee23-4653-9b16-0c35599be72c",
   "metadata": {},
   "source": [
    "Use the OpenAQ downloaded data to append data for 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73474853-879f-4159-a71b-320abded8968",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|███████████████████                        | 19/43 [02:11<02:33,  6.41s/it]/tmp/ipykernel_429968/19143918.py:59: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_all = pd.concat([df_all, df_city_2025])\n",
      " 56%|████████████████████████                   | 24/43 [02:45<02:20,  7.40s/it]/tmp/ipykernel_429968/19143918.py:59: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_all = pd.concat([df_all, df_city_2025])\n",
      " 63%|███████████████████████████                | 27/43 [03:04<01:53,  7.07s/it]/tmp/ipykernel_429968/19143918.py:59: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_all = pd.concat([df_all, df_city_2025])\n",
      "100%|███████████████████████████████████████████| 43/43 [05:11<00:00,  7.24s/it]\n"
     ]
    }
   ],
   "source": [
    "POLLUTANTS = ['pm25', 'no2', 'o3']\n",
    "\n",
    "# Step 1: Generate the 2025 hourly date range (timezone-naive)\n",
    "date_range_2025 = pd.date_range(start='2025-01-01', end='2025-12-31 23:00:00', freq='h')\n",
    "\n",
    "# Step 2: Loop over just one city for testing\n",
    "for city in tqdm(ONTARIO_CITIES):\n",
    "    city_dir = f'../../data/raw/ontario/2025/{city}'\n",
    "\n",
    "    # Create empty DataFrame template\n",
    "    columns = ['pm25', 'no2', 'o3', 'pm25_3h_avg', 'no2_3h_avg', 'o3_3h_avg', 'aqhi']\n",
    "    df_city_2025 = pd.DataFrame(index=date_range_2025, columns=columns)\n",
    "\n",
    "    # Step 3: Process raw data files for the city\n",
    "    if os.path.isdir(city_dir):  # Skip cities without data directory\n",
    "        for i in range(1, 8):  # Files 1 to 7\n",
    "            file_path = os.path.join(city_dir, f'2025-{i}.csv')\n",
    "            if not os.path.exists(file_path):\n",
    "                continue\n",
    "    \n",
    "            try:\n",
    "                df_raw = pd.read_csv(file_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file_path}: {e}\")\n",
    "                continue\n",
    "    \n",
    "            # Filter for relevant pollutants\n",
    "            df_filtered = df_raw[df_raw['parameter'].isin(POLLUTANTS)]\n",
    "    \n",
    "            # Iterate through rows and assign values\n",
    "            for _, row in df_filtered.iterrows():\n",
    "                try:\n",
    "                    # Parse and strip timezone to make it match df index\n",
    "                    ts = pd.to_datetime(row['datetimeLocal']).tz_localize(None)\n",
    "    \n",
    "                    if ts.year != 2025:\n",
    "                        continue  # Skip anything not in 2025\n",
    "    \n",
    "                    pollutant = row['parameter'].lower()\n",
    "                    value = row['value']\n",
    "    \n",
    "                    # Multiply NO2 and O3 by 1000\n",
    "                    if pollutant in ['no2', 'o3']:\n",
    "                        value *= 1000\n",
    "    \n",
    "                    df_city_2025.at[ts, pollutant] = value\n",
    "                except Exception as e:\n",
    "                    print(f\"Error parsing row: {e}\")\n",
    "                    continue\n",
    "    \n",
    "    # Step 4: Read processed historical file and insert 2025\n",
    "    processed_path = f'../../data/processed/ontario/hourly/{city}.csv'\n",
    "    \n",
    "    if os.path.exists(processed_path):\n",
    "        df_all = pd.read_csv(processed_path, index_col=0, parse_dates=True)\n",
    "        # Drop any existing 2025 rows\n",
    "        df_all = df_all[~((df_all.index >= '2025-01-01') & (df_all.index <= '2025-12-31 23:00:00'))]\n",
    "        # Append 2025\n",
    "        df_all = pd.concat([df_all, df_city_2025])\n",
    "    else:\n",
    "        # If no existing file, just use 2025 data\n",
    "        df_all = df_city_2025\n",
    "\n",
    "    # Sort index just in case\n",
    "    df_all.sort_index(inplace=True)\n",
    "\n",
    "    # Save updated DataFrame\n",
    "    df_all.to_csv(processed_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f55f6a-2609-4df9-8f78-6a561eb600d5",
   "metadata": {},
   "source": [
    "## Retrieve metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7e5376-9885-4b6a-8f85-fbd0be937d8e",
   "metadata": {},
   "source": [
    "Use 2024 PM2.5 data to retrieve metadata on location and other details for each station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afe0b06-4243-420d-8eb4-528bfb076f2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf42a39a-cab0-4895-9d13-d3edfe8554a4",
   "metadata": {},
   "source": [
    "## Compute rolling averages and AQHI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a4e10e-d0a2-425c-ada8-647addef14f1",
   "metadata": {},
   "source": [
    "Compute and update CSVs with 3-hour rolling averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0a3aea27-eb9e-44f6-8ed7-4a3c900d8b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 43/43 [00:42<00:00,  1.02it/s]\n"
     ]
    }
   ],
   "source": [
    "POLLUTANTS = ['pm25', 'no2', 'o3']\n",
    "\n",
    "for city in tqdm(ONTARIO_CITIES):\n",
    "    city_path = f'../../data/processed/ontario/hourly/{city}.csv'\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(city_path, index_col=0, parse_dates=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {city_path}: {e}\")\n",
    "        continue\n",
    "\n",
    "    for pollutant in POLLUTANTS:\n",
    "        avg_col = f\"{pollutant}_3h_avg\"\n",
    "        df[avg_col] = (\n",
    "            df[pollutant]\n",
    "            .rolling(window=3, min_periods=1)\n",
    "            .mean()\n",
    "            .round(1)  \n",
    "        )\n",
    "\n",
    "    # Save updated DataFrame\n",
    "    df.to_csv(city_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868ee538-2ad1-426d-8b3e-433ec86994d0",
   "metadata": {},
   "source": [
    "Compute and update CSVs with AQHI per-hour using rolling averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6622b763-4309-4b88-810c-5ed30667502a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 38/38 [01:02<00:00,  1.64s/it]\n"
     ]
    }
   ],
   "source": [
    "def compute_mAQI(no2, o3):\n",
    "    \"\"\"Compute Ontario mAQI from 1-hour NO2 (ppb) and O3 (ppb).\"\"\"\n",
    "    \n",
    "    def sub_index_no2(val):\n",
    "        if pd.isna(val):\n",
    "            return np.nan\n",
    "        if 0 <= val <= 110:\n",
    "            return 0.02264 * val + 1.000\n",
    "        elif 111 <= val <= 200:\n",
    "            return 0.03360 * val - 0.2291\n",
    "        elif 201 <= val <= 524:\n",
    "            return 0.01235 * val + 4.017\n",
    "        else:  # > 524\n",
    "            return 0.01810 * val + 1.000\n",
    "\n",
    "    def sub_index_o3(val):\n",
    "        if pd.isna(val):\n",
    "            return np.nan\n",
    "        if 0 <= val <= 50:\n",
    "            return 0.04980 * val + 1.000\n",
    "        elif 51 <= val <= 80:\n",
    "            return 0.1031 * val - 1.758\n",
    "        elif 81 <= val <= 149:\n",
    "            return 0.05868 * val + 1.747\n",
    "        else:  # > 149\n",
    "            return 0.05868 * val + 1.747\n",
    "\n",
    "    no2_idx = no2.apply(sub_index_no2)\n",
    "    o3_idx = o3.apply(sub_index_o3)\n",
    "\n",
    "    return pd.concat([no2_idx, o3_idx], axis=1).max(axis=1)  # keep decimals for AQHI+ comparison\n",
    "\n",
    "def compute_aqhi(pm25_3h, no2_3h, o3_3h):\n",
    "    \"\"\"Compute federal AQHI from 3-hour averages.\"\"\"\n",
    "    aqhi = (\n",
    "        1000 * (\n",
    "            (np.exp(0.000871 * no2_3h) - 1) +\n",
    "            (np.exp(0.000537 * o3_3h) - 1) +\n",
    "            (np.exp(0.000487 * pm25_3h) - 1)\n",
    "        )\n",
    "    ) / 10.4\n",
    "    return aqhi\n",
    "\n",
    "for city in tqdm(ONTARIO_CITIES):\n",
    "    city_path = f'../../data/processed/ontario/hourly/{city}.csv'\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(city_path, index_col=0, parse_dates=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {city_path}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # --- Compute mAQI from 1-hour O3 & NO2 ---\n",
    "    mAQI = compute_mAQI(df['no2'], df['o3'])\n",
    "\n",
    "    # --- Compute federal AQHI from 3-hour averages ---\n",
    "    aqhi_raw = compute_aqhi(df['pm25_3h_avg'], df['no2_3h_avg'], df['o3_3h_avg'])\n",
    "\n",
    "    # Round AQHI for reporting (per guidelines)\n",
    "    aqhi_report = aqhi_raw.round().astype('Int64')\n",
    "    \n",
    "    # Cap AQHI values for public display (11 == \"10+\")\n",
    "    aqhi_report = aqhi_report.clip(upper=11)\n",
    "\n",
    "    # --- Compute AQHI Plus ---\n",
    "    aqhi_plus = aqhi_report.copy()\n",
    "    \n",
    "    # Step 1: Apply mAQI substitution\n",
    "    mask_maqi = (mAQI > 6) & ((mAQI > aqhi_raw) | aqhi_raw.isna())\n",
    "    aqhi_plus[mask_maqi] = mAQI[mask_maqi].round().astype('Int64')\n",
    "    \n",
    "    # Step 2: Apply PM2.5 trigger (April 2024 change)\n",
    "    # pm25 here is the 1-hour value in µg/m³\n",
    "    sub_pm25 = np.ceil(df['pm25'] / 10)\n",
    "    \n",
    "    mask_pm25 = sub_pm25 > aqhi_plus\n",
    "    aqhi_plus[mask_pm25] = sub_pm25[mask_pm25].astype('Int64')\n",
    "\n",
    "    # Cap AQHI Plus values for public display (11 == \"10+\")\n",
    "    aqhi_plus = aqhi_plus.clip(upper=11)\n",
    "\n",
    "    # --- Store results ---\n",
    "    df['aqhi'] = aqhi_report\n",
    "    df['aqhi_plus'] = aqhi_plus\n",
    "\n",
    "    # Save updated file\n",
    "    df.to_csv(city_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

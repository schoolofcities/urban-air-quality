{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cdef680-d4ed-4b04-9d06-d78f57c9cfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "from calendar import monthrange\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e0411c1-76e6-435c-a98f-d4dfe7b46530",
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import ONTARIO_CITIES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9952f05b-52c4-40c8-981a-2e59c33a9009",
   "metadata": {},
   "source": [
    "## Parse and organize the pollutant data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4558a04e-37b8-49d7-affb-43be50fd974d",
   "metadata": {},
   "source": [
    "Create each of the output files with empty values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "894d66ea-4fd4-403b-adb0-0f3a7b85125c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 38/38 [00:15<00:00,  2.40it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create date range for every hour from 2003 to 2024, and empty df\n",
    "date_range = pd.date_range(start='2003-01-01', end='2024-12-31 23:00:00', freq='h')\n",
    "\n",
    "columns = ['pm25', 'no2', 'o3', 'pm25_3h_avg', 'no2_3h_avg', 'o3_3h_avg', 'aqhi_raw', 'aqhi_plus_raw', 'aqhi', 'aqhi_plus']\n",
    "df_empty = pd.DataFrame(index=date_range, columns=columns)\n",
    "\n",
    "# Save a copy for each city\n",
    "output_dir = '../../data/processed/ontario/hourly/'\n",
    "for city in tqdm(ONTARIO_CITIES):\n",
    "    filename = f\"{output_dir}{city}.csv\"\n",
    "    df_empty.to_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8787e707-fcae-42de-ae62-e182fc2a1e0f",
   "metadata": {},
   "source": [
    "Parse data for each year and each pollutant from 2005-2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15acda9d-099a-4bb8-b039-5c13b3506a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_city_df(city_name):\n",
    "    path = f'../../data/processed/ontario/hourly/{city_name}.csv'\n",
    "    return pd.read_csv(path, index_col=0, parse_dates=True)\n",
    "\n",
    "\n",
    "def parse_city_block(city_data_rows, header_row):\n",
    "    try:\n",
    "        header_trimmed = header_row[:27]\n",
    "        rows_trimmed = [row[:27] for row in city_data_rows]\n",
    "\n",
    "        df = pd.DataFrame(rows_trimmed)\n",
    "        df.columns = header_trimmed\n",
    "        df = df[['Date'] + [f'H{h:02d}' for h in range(1, 25)]]\n",
    "        df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "        df = df.dropna(subset=['Date'])\n",
    "\n",
    "        melted = df.melt(id_vars='Date', var_name='hour', value_name='value')\n",
    "        melted['hour'] = melted['hour'].str.extract(r'H(\\d{2})').astype(int)  # No -1 since H1 is 1AM\n",
    "        melted['datetime'] = melted['Date'] + pd.to_timedelta(melted['hour'], unit='h') + pd.Timedelta(hours=1)  # +1 since H1 is 1AM is 1:00-1:59AM stored at 2AM\n",
    "        melted.set_index('datetime', inplace=True)\n",
    "\n",
    "        melted['value'] = pd.to_numeric(melted['value'], errors='coerce')\n",
    "        melted.loc[melted['value'].isin([-999, 9999]), 'value'] = np.nan\n",
    "\n",
    "        return melted\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing city block: {e}\")\n",
    "        return None\n",
    "\n",
    "def update_city_df(city_df, melted, pollutant):\n",
    "    try:\n",
    "        # Clip melted index to only those within the city's df index\n",
    "        melted = melted[melted.index.isin(city_df.index)]\n",
    "\n",
    "        city_df.loc[melted.index, pollutant] = melted['value']\n",
    "        return city_df\n",
    "    except Exception as e:\n",
    "        print(f\"Error updating city DataFrame: {e}\")\n",
    "        return city_df\n",
    "\n",
    "\n",
    "def process_city_block(city_name, header_row, city_data_rows, pollutant):\n",
    "    try:\n",
    "        city_df = load_city_df(city_name)\n",
    "        melted = parse_city_block(city_data_rows, header_row)\n",
    "        if melted is not None:\n",
    "            updated_df = update_city_df(city_df, melted, pollutant)\n",
    "            updated_df.to_csv(f'../../data/processed/ontario/hourly/{city_name}.csv')\n",
    "            # print(f\"✅ Updated {city_name}.csv with {pollutant} data.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to process {city_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00038570-ea3e-4e90-88cb-aa2c6c6fcddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 22/22 [13:54<00:00, 37.92s/it]\n",
      "100%|███████████████████████████████████████████| 22/22 [15:15<00:00, 41.61s/it]\n",
      "100%|███████████████████████████████████████████| 22/22 [15:00<00:00, 40.95s/it]\n"
     ]
    }
   ],
   "source": [
    "pollutants = ['pm25', 'o3', 'no2']\n",
    "years = list(range(2003, 2025))\n",
    "\n",
    "for pollutant in pollutants:\n",
    "    for year in tqdm(years):\n",
    "        file_path = f'../../data/raw/ontario/{pollutant}/{year}_{pollutant}.csv'\n",
    "\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"⚠️ File not found: {file_path}\")\n",
    "            continue\n",
    "\n",
    "        with open(file_path, newline='', encoding='utf-8') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            current_city = None\n",
    "            header_row = []\n",
    "            city_data_rows = []\n",
    "\n",
    "            for row in reader:\n",
    "                if not row:\n",
    "                    continue\n",
    "\n",
    "                first_cell = row[0].strip().lower()\n",
    "\n",
    "                if first_cell in ONTARIO_CITIES:\n",
    "                    # Process the previous block if exists\n",
    "                    if current_city and city_data_rows and header_row:\n",
    "                        try:\n",
    "                            process_city_block(current_city, header_row, city_data_rows, pollutant)\n",
    "                        except Exception as e:\n",
    "                            print(f\"❌ Failed to process {current_city} in {file_path}: {e}\")\n",
    "\n",
    "                    # Start a new block\n",
    "                    current_city = first_cell\n",
    "                    header_row = []\n",
    "                    city_data_rows = []\n",
    "                    continue\n",
    "\n",
    "                if current_city:\n",
    "                    if len(row) > 2 and row[2].strip().lower() == 'date':\n",
    "                        header_row = row\n",
    "                    elif header_row and len(row) >= 27:\n",
    "                        city_data_rows.append(row)\n",
    "\n",
    "            # Process last city block after file ends\n",
    "            if current_city and city_data_rows and header_row:\n",
    "                try:\n",
    "                    process_city_block(current_city, header_row, city_data_rows, pollutant)\n",
    "                except Exception as e:\n",
    "                    print(f\"❌ Failed to process final block for {current_city} in {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed247e66-ee23-4653-9b16-0c35599be72c",
   "metadata": {},
   "source": [
    "Use the OpenAQ downloaded data to append data for 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73474853-879f-4159-a71b-320abded8968",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 38/38 [05:11<00:00,  8.20s/it]\n"
     ]
    }
   ],
   "source": [
    "POLLUTANTS = ['pm25', 'no2', 'o3']\n",
    "\n",
    "# Step 1: Generate the 2025 hourly date range (timezone-naive)\n",
    "date_range_2025 = pd.date_range(start='2025-01-01', end='2025-12-31 23:00:00', freq='h')\n",
    "\n",
    "# Step 2: Loop over just one city for testing\n",
    "for city in tqdm(ONTARIO_CITIES):\n",
    "    city_dir = f'../../data/raw/ontario/2025/{city}'\n",
    "\n",
    "    # Create empty DataFrame template\n",
    "    columns = ['pm25', 'no2', 'o3', 'pm25_3h_avg', 'no2_3h_avg', 'o3_3h_avg', 'aqhi']\n",
    "    df_city_2025 = pd.DataFrame(index=date_range_2025, columns=columns)\n",
    "\n",
    "    # Step 3: Process raw data files for the city\n",
    "    if os.path.isdir(city_dir):  # Skip cities without data directory\n",
    "        for i in range(1, 8):  # Files 1 to 7\n",
    "            file_path = os.path.join(city_dir, f'2025-{i}.csv')\n",
    "            if not os.path.exists(file_path):\n",
    "                continue\n",
    "    \n",
    "            try:\n",
    "                df_raw = pd.read_csv(file_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file_path}: {e}\")\n",
    "                continue\n",
    "    \n",
    "            # Filter for relevant pollutants\n",
    "            df_filtered = df_raw[df_raw['parameter'].isin(POLLUTANTS)]\n",
    "    \n",
    "            # Iterate through rows and assign values\n",
    "            for _, row in df_filtered.iterrows():\n",
    "                try:\n",
    "                    # Parse and strip timezone to make it match df index\n",
    "                    ts = pd.to_datetime(row['datetimeLocal']).tz_localize(None)\n",
    "    \n",
    "                    if ts.year != 2025:\n",
    "                        continue  # Skip anything not in 2025\n",
    "    \n",
    "                    pollutant = row['parameter'].lower()\n",
    "                    value = row['value']\n",
    "    \n",
    "                    # Multiply NO2 and O3 by 1000\n",
    "                    if pollutant in ['no2', 'o3']:\n",
    "                        value *= 1000\n",
    "    \n",
    "                    df_city_2025.at[ts, pollutant] = value\n",
    "                except Exception as e:\n",
    "                    print(f\"Error parsing row: {e}\")\n",
    "                    continue\n",
    "    \n",
    "    # Step 4: Read processed historical file and insert 2025\n",
    "    processed_path = f'../../data/processed/ontario/hourly/{city}.csv'\n",
    "    \n",
    "    if os.path.exists(processed_path):\n",
    "        df_all = pd.read_csv(processed_path, index_col=0, parse_dates=True)\n",
    "        # Drop any existing 2025 rows\n",
    "        df_all = df_all[~((df_all.index >= '2025-01-01') & (df_all.index <= '2025-12-31 23:00:00'))]\n",
    "        # Append 2025\n",
    "        df_all = pd.concat([df_all, df_city_2025])\n",
    "    else:\n",
    "        # If no existing file, just use 2025 data\n",
    "        df_all = df_city_2025\n",
    "\n",
    "    # Sort index just in case\n",
    "    df_all.sort_index(inplace=True)\n",
    "\n",
    "    # Save updated DataFrame\n",
    "    df_all.to_csv(processed_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf42a39a-cab0-4895-9d13-d3edfe8554a4",
   "metadata": {},
   "source": [
    "## Compute rolling averages and AQHI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a4e10e-d0a2-425c-ada8-647addef14f1",
   "metadata": {},
   "source": [
    "Compute and update CSVs with 3-hour rolling averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a3aea27-eb9e-44f6-8ed7-4a3c900d8b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 38/38 [00:43<00:00,  1.14s/it]\n"
     ]
    }
   ],
   "source": [
    "POLLUTANTS = ['pm25', 'no2', 'o3']\n",
    "\n",
    "for city in tqdm(ONTARIO_CITIES):\n",
    "    city_path = f'../../data/processed/ontario/hourly/{city}.csv'\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(city_path, index_col=0, parse_dates=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {city_path}: {e}\")\n",
    "        continue\n",
    "\n",
    "    for pollutant in POLLUTANTS:\n",
    "        avg_col = f\"{pollutant}_3h_avg\"\n",
    "        df[avg_col] = (\n",
    "            df[pollutant]\n",
    "            .rolling(window=3, min_periods=1)\n",
    "            .mean()\n",
    "            .round(1)  \n",
    "        )\n",
    "\n",
    "    # Save updated DataFrame\n",
    "    df.to_csv(city_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868ee538-2ad1-426d-8b3e-433ec86994d0",
   "metadata": {},
   "source": [
    "Compute and update CSVs with AQHI per-hour using rolling averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6622b763-4309-4b88-810c-5ed30667502a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 38/38 [00:47<00:00,  1.26s/it]\n"
     ]
    }
   ],
   "source": [
    "def compute_mAQI(no2, o3):\n",
    "    \"\"\"Compute Ontario mAQI from 1-hour NO2 (ppb) and O3 (ppb).\"\"\"\n",
    "    \n",
    "    def sub_index_no2(val):\n",
    "        if pd.isna(val):\n",
    "            return np.nan\n",
    "        if 0 <= val <= 110:\n",
    "            return 0.02264 * val + 1.000\n",
    "        elif 111 <= val <= 200:\n",
    "            return 0.03360 * val - 0.2291\n",
    "        elif 201 <= val <= 524:\n",
    "            return 0.01235 * val + 4.017\n",
    "        else:  # > 524\n",
    "            return 0.01810 * val + 1.000\n",
    "\n",
    "    def sub_index_o3(val):\n",
    "        if pd.isna(val):\n",
    "            return np.nan\n",
    "        if 0 <= val <= 50:\n",
    "            return 0.04980 * val + 1.000\n",
    "        elif 51 <= val <= 80:\n",
    "            return 0.1031 * val - 1.758\n",
    "        elif 81 <= val <= 149:\n",
    "            return 0.05868 * val + 1.747\n",
    "        else:  # > 149\n",
    "            return 0.05868 * val + 1.747\n",
    "\n",
    "    no2_idx = no2.apply(sub_index_no2)\n",
    "    o3_idx = o3.apply(sub_index_o3)\n",
    "\n",
    "    return pd.concat([no2_idx, o3_idx], axis=1).max(axis=1)  # keep decimals for AQHI+ comparison\n",
    "\n",
    "def compute_aqhi(pm25_3h, no2_3h, o3_3h):\n",
    "    \"\"\"Compute federal AQHI from 3-hour averages.\"\"\"\n",
    "    aqhi = (\n",
    "        1000 * (\n",
    "            (np.exp(0.000871 * no2_3h) - 1) +\n",
    "            (np.exp(0.000537 * o3_3h) - 1) +\n",
    "            (np.exp(0.000487 * pm25_3h) - 1)\n",
    "        )\n",
    "    ) / 10.4\n",
    "    return aqhi\n",
    "\n",
    "for city in tqdm(ONTARIO_CITIES):\n",
    "    city_path = f'../../data/processed/ontario/hourly/{city}.csv'\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(city_path, index_col=0, parse_dates=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {city_path}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # --- Compute mAQI from 1-hour O3 & NO2 ---\n",
    "    mAQI = compute_mAQI(df['no2'], df['o3'])\n",
    "\n",
    "    # --- Compute federal AQHI from 3-hour averages ---\n",
    "    aqhi_raw = compute_aqhi(df['pm25_3h_avg'], df['no2_3h_avg'], df['o3_3h_avg'])\n",
    "\n",
    "    # Round AQHI for reporting (per guidelines)\n",
    "    aqhi_report = aqhi_raw.round().astype('Int64')\n",
    "    \n",
    "    # Cap AQHI values for public display (11 == \"10+\")\n",
    "    aqhi_report_final = aqhi_report.clip(upper=11)\n",
    "\n",
    "    # --- Compute AQHI Plus ---\n",
    "    aqhi_plus = aqhi_report.copy()\n",
    "    \n",
    "    # Step 1: Apply mAQI substitution\n",
    "    mask_maqi = (mAQI > 6) & ((mAQI > aqhi_raw) | aqhi_raw.isna())\n",
    "    aqhi_plus[mask_maqi] = mAQI[mask_maqi].round().astype('Int64')\n",
    "    \n",
    "    # Step 2: Apply PM2.5 trigger (April 2024 change)\n",
    "    # pm25 here is the 1-hour value in µg/m³\n",
    "    sub_pm25 = np.ceil(df['pm25'] / 10)\n",
    "    \n",
    "    mask_pm25 = sub_pm25 > aqhi_plus\n",
    "    aqhi_plus[mask_pm25] = sub_pm25[mask_pm25].astype('Int64')\n",
    "\n",
    "    # Cap AQHI Plus values for public display (11 == \"10+\")\n",
    "    aqhi_plus_final = aqhi_plus.clip(upper=11)\n",
    "\n",
    "    # --- Store results ---\n",
    "    df['aqhi_raw'] = aqhi_report\n",
    "    df['aqhi'] = aqhi_report_final\n",
    "    df['aqhi_plus_raw'] = aqhi_plus\n",
    "    df['aqhi_plus'] = aqhi_plus_final\n",
    "\n",
    "    # Save updated file\n",
    "    df.to_csv(city_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6020414-cf3f-421c-beb7-e40bed40ebd7",
   "metadata": {},
   "source": [
    "## Compute daily AQHI values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be261040-20b5-4c30-847b-ac0e676d3aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 38/38 [03:32<00:00,  5.60s/it]\n"
     ]
    }
   ],
   "source": [
    "daily_dates = pd.date_range(start=\"2005-01-01\", end=\"2025-12-31\", freq=\"D\")\n",
    "\n",
    "# Loop through cities\n",
    "for city in tqdm(ONTARIO_CITIES):\n",
    "    hourly_path = f'../../data/processed/ontario/hourly/{city}.csv'\n",
    "    daily_path = f'../../data/processed/ontario/daily/{city}.csv'\n",
    "\n",
    "    if not os.path.exists(hourly_path):\n",
    "        print(f\"No hourly file for {city}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        df_hourly = pd.read_csv(hourly_path, index_col=0, parse_dates=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {hourly_path}: {e}\")\n",
    "        continue\n",
    "\n",
    "    if 'aqhi_plus' not in df_hourly.columns:\n",
    "        print(f\"No 'aqhi_plus' column for {city}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Create empty daily DataFrame\n",
    "    df_daily = pd.DataFrame(index=daily_dates, columns=['4pm_aqhi', 'min_aqhi', 'max_aqhi', 'mean_aqhi', 'med_aqhi'], dtype=float)\n",
    "\n",
    "    # Group hourly data by date\n",
    "    grouped = df_hourly.groupby(df_hourly.index.date)\n",
    "\n",
    "    for date, group in grouped:\n",
    "        date = pd.Timestamp(date)\n",
    "\n",
    "        # 4pm AQHI Plus (16:00 hour if present)\n",
    "        four_pm_val = group.loc[group.index.hour == 16, 'aqhi_plus']\n",
    "        df_daily.at[date, '4pm_aqhi'] = four_pm_val.iloc[0] if not four_pm_val.empty else np.nan\n",
    "\n",
    "        # Daily min, max, median\n",
    "        aqhi_vals = group['aqhi_plus'].dropna()\n",
    "        aqhi_vals_raw = group['aqhi_plus_raw'].dropna()\n",
    "        if not aqhi_vals.empty:\n",
    "            df_daily.at[date, 'min_aqhi'] = aqhi_vals.min()\n",
    "            df_daily.at[date, 'max_aqhi'] = aqhi_vals.max()\n",
    "            df_daily.at[date, 'mean_aqhi'] = np.clip(aqhi_vals_raw.mean().round(1), None, 11)\n",
    "            df_daily.at[date, 'med_aqhi'] = aqhi_vals.median()\n",
    "\n",
    "    # Save daily file\n",
    "    df_daily.to_csv(daily_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfb8c67-8606-4cad-b053-c118efc9f242",
   "metadata": {},
   "source": [
    "## Retrieve metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b90e2c0-8b10-4c8f-9a59-5a950fb65e39",
   "metadata": {},
   "source": [
    "Use 2024 PM2.5 data to retrieve metadata on location and other details for each station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afe0b06-4243-420d-8eb4-528bfb076f2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "618fe349-8e18-4c37-825a-c5ebc59e8b90",
   "metadata": {},
   "source": [
    "Compute the coverage of the data - percent of each pollutant in each month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4078a172-2bca-42e9-944f-b9c4207cfa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coverage_for_city(city_name):\n",
    "    # Load processed city data\n",
    "    path = f'../../data/processed/ontario/hourly/{city_name}.csv'\n",
    "    df = pd.read_csv(path, index_col=0, parse_dates=True)\n",
    "\n",
    "    # Ensure all required columns are present\n",
    "    required_cols = ['pm25', 'no2', 'o3',\n",
    "                     'pm25_3h_avg', 'no2_3h_avg', 'o3_3h_avg',\n",
    "                     'aqhi', 'aqhi_plus', 'aqhi_raw', 'aqhi_plus_raw']\n",
    "    for col in required_cols:\n",
    "        if col not in df.columns:\n",
    "            df[col] = np.nan\n",
    "\n",
    "    # Build full month index from 2003–2025\n",
    "    month_index = pd.date_range(start='2003-01-01', end='2025-12-31', freq='MS')\n",
    "    coverage = pd.DataFrame(index=month_index, columns=required_cols)\n",
    "\n",
    "    # Compute coverage per column, per month\n",
    "    for month_start in month_index:\n",
    "        month_end = month_start + pd.offsets.MonthEnd(1)\n",
    "        month_hours = pd.date_range(start=month_start, end=month_end, freq='h')\n",
    "        expected_count = len(month_hours)\n",
    "\n",
    "        for col in required_cols:\n",
    "            if month_start < df.index.min() or month_start > df.index.max():\n",
    "                coverage.loc[month_start, col] = 0.00\n",
    "            else:\n",
    "                mask = (df.index >= month_start) & (df.index <= month_end)\n",
    "                observed_count = df.loc[mask, col].notna().sum()\n",
    "                coverage.loc[month_start, col] = round((observed_count / expected_count) * 100, 2)\n",
    "\n",
    "    return coverage\n",
    "\n",
    "\n",
    "# Example: compute coverage for Toronto Downtown\n",
    "coverage_toronto = compute_coverage_for_city('toronto downtown')\n",
    "coverage_toronto.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil import parser\n",
    "from calendar import monthrange\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from openaq import OpenAQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import FEDERAL_LOCATION_IDS\n",
    "\n",
    "FEDERAL_CITIES = list(FEDERAL_LOCATION_IDS.keys())\n",
    "years = list(range(2005, 2026))\n",
    "\n",
    "COVERAGE_DIR = \"../../data/processed/federal/metadata/coverage/yearly\"\n",
    "COVERAGE_THRESHOLD = 50  # percent\n",
    "\n",
    "# Define only the ranges we need for the CSV outputs\n",
    "categories_to_save = {\n",
    "    \"aqhi\": {\n",
    "        \"7plus\": (7, np.inf),\n",
    "        \"11plus\": (11, np.inf),\n",
    "    },\n",
    "    \"aqhi_plus\": {\n",
    "        \"7plus\": (7, np.inf),\n",
    "        \"11plus\": (11, np.inf),\n",
    "    },\n",
    "    \"pm25\": {\n",
    "        \"55_5plus\": (55.5, np.inf),\n",
    "        \"225_5plus\": (225.5, np.inf),\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_num_days(case, col, categories):\n",
    "    \"\"\"\n",
    "    Compute number of days per city per year falling within each category.\n",
    "    Only include years where coverage >= 50% for the pollutant/metric.\n",
    "    Returns a dict of {category_name: DataFrame(cities x years)}.\n",
    "    \"\"\"\n",
    "    results = {cat: pd.DataFrame(0, index=FEDERAL_CITIES, columns=years) for cat in categories}\n",
    "\n",
    "    print(f\"Processing {case}\")\n",
    "    for city in tqdm(FEDERAL_CITIES):\n",
    "        dir_end = case\n",
    "        if case == \"aqhi_plus\":\n",
    "            dir_end = \"aqhi-plus\"\n",
    "\n",
    "        daily_path = f\"../../data/processed/federal/daily-{dir_end}/{city}.csv\"\n",
    "        coverage_path = f\"{COVERAGE_DIR}/{city}.csv\"\n",
    "\n",
    "        # Skip if either file missing\n",
    "        if not os.path.exists(daily_path) or not os.path.exists(coverage_path):\n",
    "            continue\n",
    "\n",
    "        # Load daily data\n",
    "        df_daily = pd.read_csv(daily_path, index_col=0, parse_dates=True)\n",
    "        if col not in df_daily.columns:\n",
    "            continue\n",
    "        series = pd.to_numeric(df_daily[col], errors=\"coerce\").dropna()\n",
    "        if series.empty:\n",
    "            continue\n",
    "\n",
    "        # Load coverage metadata\n",
    "        try:\n",
    "            df_cov = pd.read_csv(coverage_path, index_col=0)\n",
    "            # Identify valid years with >= 50% coverage for this metric\n",
    "            valid_years = df_cov.index[df_cov[case] >= COVERAGE_THRESHOLD].astype(int).tolist()\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Skipping {city} (coverage load error: {e})\")\n",
    "            continue\n",
    "\n",
    "        for cat_name, (low, high) in categories.items():\n",
    "            mask = (series >= low) & (series <= high if np.isfinite(high) else True)\n",
    "            filtered = series[mask]\n",
    "            if filtered.empty:\n",
    "                continue\n",
    "\n",
    "            yearly_counts = filtered.groupby(filtered.index.year).size()\n",
    "\n",
    "            # Only record counts for valid coverage years\n",
    "            for year, count in yearly_counts.items():\n",
    "                results[cat_name].loc[city, year] = count\n",
    "\n",
    "            for year in years:\n",
    "                if year not in valid_years:\n",
    "                    results[cat_name].loc[city, year] = np.nan\n",
    "\n",
    "    for cat in results:\n",
    "        results[cat] = results[cat].rename(index={'Metro Van - Vancouver': 'Vancouver'})\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing aqhi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 42/42 [00:00<00:00, 73.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing aqhi_plus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 42/42 [00:00<00:00, 61.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing pm25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 42/42 [00:00<00:00, 64.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Finished generating all 6 CSV files!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Output directory\n",
    "out_dir = \"../../data/results/federal_num_days\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# ---- AQHI ----\n",
    "aqhi_results = compute_num_days(\"aqhi\", \"max_aqhi\", categories_to_save[\"aqhi\"])\n",
    "for cat, df in aqhi_results.items():\n",
    "    df.to_csv(f\"{out_dir}/aqhi_{cat}.csv\")\n",
    "\n",
    "# ---- AQHI+ ----\n",
    "aqhi_plus_results = compute_num_days(\"aqhi_plus\", \"max_aqhi_plus\", categories_to_save[\"aqhi_plus\"])\n",
    "for cat, df in aqhi_plus_results.items():\n",
    "    df.to_csv(f\"{out_dir}/aqhi_plus_{cat}.csv\")\n",
    "\n",
    "# ---- PM2.5 ----\n",
    "pm25_results = compute_num_days(\"pm25\", \"max_pm25\", categories_to_save[\"pm25\"])\n",
    "for cat, df in pm25_results.items():\n",
    "    df.to_csv(f\"{out_dir}/pm25_{cat}.csv\")\n",
    "\n",
    "print(\"âœ… Finished generating all 6 CSV files!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved: ../../data/results/federal_num_days/pm25_55_5plus.json\n",
      "âœ… Saved: ../../data/results/federal_num_days/pm25_225_5plus.json\n",
      "âœ… Saved: ../../data/results/federal_num_days/pm25_250_5plus.json\n",
      "âœ… Saved: ../../data/results/federal_num_days/aqhi_7plus.json\n",
      "âœ… Saved: ../../data/results/federal_num_days/aqhi_plus_7plus.json\n",
      "âœ… Saved: ../../data/results/federal_num_days/aqhi_plus_11plus.json\n",
      "âœ… Saved: ../../data/results/federal_num_days/aqhi_11plus.json\n",
      "ðŸŽ‰ All summary JSONs generated successfully.\n"
     ]
    }
   ],
   "source": [
    "results_dir = \"../../data/results/federal_num_days\"\n",
    "output_dir = results_dir\n",
    "\n",
    "# Loop through each CSV file\n",
    "for csv_file in os.listdir(results_dir):\n",
    "    if not csv_file.endswith(\".csv\"):\n",
    "        continue\n",
    "\n",
    "    csv_path = os.path.join(results_dir, csv_file)\n",
    "    json_path = os.path.join(output_dir, csv_file.replace(\".csv\", \".json\"))\n",
    "\n",
    "    # Read CSV with city as index (first column)\n",
    "    df = pd.read_csv(csv_path, index_col=0)\n",
    "\n",
    "    # Replace NaN and NaT with None (JSON null)\n",
    "    df = df.replace({np.nan: None})\n",
    "\n",
    "    # Convert to dict of dicts â€” {city: {col1: val1, col2: val2, ...}}\n",
    "    json_dict = df.to_dict(orient=\"index\")\n",
    "\n",
    "    # Write to JSON\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(json_dict, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"âœ… Saved: {json_path}\")\n",
    "\n",
    "print(\"ðŸŽ‰ All summary JSONs generated successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankings_base_dir = \"../../data/results/federal_num_days_rankings\"\n",
    "intervals = {\"1year\": 1, \"3year\": 3, \"5year\": 5}\n",
    "\n",
    "os.makedirs(rankings_base_dir, exist_ok=True)\n",
    "for folder in intervals:\n",
    "    os.makedirs(os.path.join(rankings_base_dir, folder), exist_ok=True)\n",
    "\n",
    "results_dir = \"../../data/results/federal_num_days\"\n",
    "\n",
    "# Categories to process (same six CSV groups)\n",
    "ranking_cases = [\n",
    "    (\"aqhi\", \"7plus\"),\n",
    "    (\"aqhi\", \"11plus\"),\n",
    "    (\"aqhi_plus\", \"7plus\"),\n",
    "    (\"aqhi_plus\", \"11plus\"),\n",
    "    (\"pm25\", \"55_5plus\"),\n",
    "    (\"pm25\", \"225_5plus\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_interval_sums(df, interval):\n",
    "    \"\"\"\n",
    "    Given a DataFrame (cities Ã— years), compute interval sums.\n",
    "    Handles year columns as strings or ints.\n",
    "    For 1-year: returns original.\n",
    "    For 3-year: sums over [2005â€“2007, 2008â€“2010, ...].\n",
    "    For 5-year: sums over [2006â€“2010, 2011â€“2015, ..., 2021â€“2025].\n",
    "    Returns a new DataFrame (cities Ã— interval labels).\n",
    "    \"\"\"\n",
    "    df_out = pd.DataFrame(index=df.index)\n",
    "\n",
    "    # Normalize column names to int\n",
    "    col_years = []\n",
    "    for c in df.columns:\n",
    "        try:\n",
    "            col_years.append(int(c))\n",
    "        except ValueError:\n",
    "            continue  # skip non-year columns\n",
    "\n",
    "    years_all = sorted([y for y in col_years if 2005 <= y <= 2025])\n",
    "\n",
    "    if interval == 1:\n",
    "        return df.rename(columns={str(y): y for y in years_all})\n",
    "\n",
    "    if interval == 3:\n",
    "        starts = list(range(2005, 2026, 3))\n",
    "    elif interval == 5:\n",
    "        starts = list(range(2006, 2026, 5))\n",
    "    else:\n",
    "        raise ValueError(\"Interval must be 1, 3, or 5\")\n",
    "\n",
    "    # Map DataFrame columns (intâ†’actual string label)\n",
    "    col_map = {int(c): c for c in df.columns if str(c).isdigit()}\n",
    "\n",
    "    for start in starts:\n",
    "        end = start + interval - 1\n",
    "        cols = [col_map[y] for y in years_all if start <= y <= end and y in col_map]\n",
    "        if not cols:\n",
    "            continue\n",
    "        df_out[f\"{start}-{end}\"] = df[cols].sum(axis=1, skipna=True)\n",
    "\n",
    "    return df_out\n",
    "\n",
    "\n",
    "def compute_rankings(df_sum):\n",
    "    \"\"\"\n",
    "    Given a DataFrame of sums (cities Ã— intervals),\n",
    "    return rankings (1 = highest value, ties share same rank).\n",
    "    \"\"\"\n",
    "    return df_sum.rank(axis=0, ascending=False, method=\"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved rankings: ../../data/results/federal_num_days_rankings/1year/aqhi_7plus_rank.csv and ../../data/results/federal_num_days_rankings/1year/aqhi_7plus_rank.json\n",
      "âœ… Saved rankings: ../../data/results/federal_num_days_rankings/3year/aqhi_7plus_rank.csv and ../../data/results/federal_num_days_rankings/3year/aqhi_7plus_rank.json\n",
      "âœ… Saved rankings: ../../data/results/federal_num_days_rankings/5year/aqhi_7plus_rank.csv and ../../data/results/federal_num_days_rankings/5year/aqhi_7plus_rank.json\n",
      "âœ… Saved rankings: ../../data/results/federal_num_days_rankings/1year/aqhi_11plus_rank.csv and ../../data/results/federal_num_days_rankings/1year/aqhi_11plus_rank.json\n",
      "âœ… Saved rankings: ../../data/results/federal_num_days_rankings/3year/aqhi_11plus_rank.csv and ../../data/results/federal_num_days_rankings/3year/aqhi_11plus_rank.json\n",
      "âœ… Saved rankings: ../../data/results/federal_num_days_rankings/5year/aqhi_11plus_rank.csv and ../../data/results/federal_num_days_rankings/5year/aqhi_11plus_rank.json\n",
      "âœ… Saved rankings: ../../data/results/federal_num_days_rankings/1year/aqhi_plus_7plus_rank.csv and ../../data/results/federal_num_days_rankings/1year/aqhi_plus_7plus_rank.json\n",
      "âœ… Saved rankings: ../../data/results/federal_num_days_rankings/3year/aqhi_plus_7plus_rank.csv and ../../data/results/federal_num_days_rankings/3year/aqhi_plus_7plus_rank.json\n",
      "âœ… Saved rankings: ../../data/results/federal_num_days_rankings/5year/aqhi_plus_7plus_rank.csv and ../../data/results/federal_num_days_rankings/5year/aqhi_plus_7plus_rank.json\n",
      "âœ… Saved rankings: ../../data/results/federal_num_days_rankings/1year/aqhi_plus_11plus_rank.csv and ../../data/results/federal_num_days_rankings/1year/aqhi_plus_11plus_rank.json\n",
      "âœ… Saved rankings: ../../data/results/federal_num_days_rankings/3year/aqhi_plus_11plus_rank.csv and ../../data/results/federal_num_days_rankings/3year/aqhi_plus_11plus_rank.json\n",
      "âœ… Saved rankings: ../../data/results/federal_num_days_rankings/5year/aqhi_plus_11plus_rank.csv and ../../data/results/federal_num_days_rankings/5year/aqhi_plus_11plus_rank.json\n",
      "âœ… Saved rankings: ../../data/results/federal_num_days_rankings/1year/pm25_55_5plus_rank.csv and ../../data/results/federal_num_days_rankings/1year/pm25_55_5plus_rank.json\n",
      "âœ… Saved rankings: ../../data/results/federal_num_days_rankings/3year/pm25_55_5plus_rank.csv and ../../data/results/federal_num_days_rankings/3year/pm25_55_5plus_rank.json\n",
      "âœ… Saved rankings: ../../data/results/federal_num_days_rankings/5year/pm25_55_5plus_rank.csv and ../../data/results/federal_num_days_rankings/5year/pm25_55_5plus_rank.json\n",
      "âœ… Saved rankings: ../../data/results/federal_num_days_rankings/1year/pm25_225_5plus_rank.csv and ../../data/results/federal_num_days_rankings/1year/pm25_225_5plus_rank.json\n",
      "âœ… Saved rankings: ../../data/results/federal_num_days_rankings/3year/pm25_225_5plus_rank.csv and ../../data/results/federal_num_days_rankings/3year/pm25_225_5plus_rank.json\n",
      "âœ… Saved rankings: ../../data/results/federal_num_days_rankings/5year/pm25_225_5plus_rank.csv and ../../data/results/federal_num_days_rankings/5year/pm25_225_5plus_rank.json\n",
      "ðŸŽ‰ All ranking CSV and JSON files generated successfully.\n"
     ]
    }
   ],
   "source": [
    "for case, cat in ranking_cases:\n",
    "    csv_name = f\"{case}_{cat}.csv\"\n",
    "    csv_path = os.path.join(results_dir, csv_name)\n",
    "\n",
    "    if not os.path.exists(csv_path):\n",
    "        print(f\"âš ï¸ Missing file: {csv_name}\")\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(csv_path, index_col=0)\n",
    "\n",
    "    # Drop completely empty rows (no data at all)\n",
    "    df = df.dropna(how=\"all\")\n",
    "\n",
    "    for label, interval in intervals.items():\n",
    "        df_sum = compute_interval_sums(df, interval)\n",
    "        df_rank = compute_rankings(df_sum)\n",
    "\n",
    "        # Save to directory\n",
    "        out_dir = os.path.join(rankings_base_dir, label)\n",
    "        csv_out = os.path.join(out_dir, f\"{case}_{cat}_rank.csv\")\n",
    "        json_out = csv_out.replace(\".csv\", \".json\")\n",
    "\n",
    "        df_rank.to_csv(csv_out)\n",
    "\n",
    "        # Convert to JSON {city: {interval: rank}}\n",
    "        json_dict = df_rank.replace({np.nan: None}).to_dict(orient=\"index\")\n",
    "\n",
    "        with open(json_out, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(json_dict, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "        print(f\"âœ… Saved rankings: {csv_out} and {json_out}\")\n",
    "\n",
    "print(\"ðŸŽ‰ All ranking CSV and JSON files generated successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
